{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 4: Bayesian Model Evaluation and Workflow\n",
    "\n",
    "In this session, we will learn to evaluate the quality of our models using statistical and visual diagnostics. We'll also discuss a comprehensive Bayesian workflow that promotes model development through an iterative process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import pymc as pm\n",
    "import pytensor.tensor as pt\n",
    "import arviz as az\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "RANDOM_SEED = 20090425"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCMC Output Processing and Model Checking with ArviZ\n",
    "\n",
    "[ArviZ](https://python.arviz.org/en/stable/) is a Python package for exploratory analysis of Bayesian models. It includes functions for posterior analysis, model checking, comparison and diagnostics. ArviZ is designed to work with output from a wide range of Bayesian inference libraries, including PyMC, emcee, Stan, Pyro, and TensorFlow Probability.\n",
    "\n",
    "ArviZ is built on top of the popular libraries xarray, matplotlib, and bokeh. It is also built with the same design principles as PyMC, so if you are familiar with PyMC, you will find ArviZ easy to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Effect of coaching on SAT scores\n",
    "\n",
    "This example was taken from Gelman *et al.* (2013):\n",
    "\n",
    "> A study was performed for the Educational Testing Service to analyze the effects of special coaching programs on test scores. Separate randomized experiments were performed to estimate the effects of coaching programs for the SAT-V (Scholastic Aptitude Test- Verbal) in each of eight high schools. The outcome variable in each study was the score on a special administration of the SAT-V, a standardized multiple choice test administered by the Educational Testing Service and used to help colleges make admissions decisions; the scores can vary between 200 and 800, with mean about 500 and standard deviation about 100. The SAT examinations are designed to be resistant to short-term efforts directed specifically toward improving performance on the test; instead they are designed to reflect knowledge acquired and abilities developed over many years of education. Nevertheless, each of the eight schools in this study considered its short-term coaching program to be successful at increasing SAT scores. Also, there was no prior reason to believe that any of the eight programs was more effective than any other or that some were more similar in effect to each other than to any other.\n",
    "\n",
    "We are given the estimated coaching effects (`y`) and their sampling variances (`s`). The estimates were obtained by independent experiments, with relatively large sample sizes (over thirty students in each school), so you it can be assumed that they have approximately normal sampling distributions with known variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([28, 8, -3, 7, -1, 1, 18, 12])\n",
    "s = np.array([15, 10, 16, 11, 9, 11, 10, 18])\n",
    "schools = np.array(\n",
    "    [\n",
    "        \"Choate\",\n",
    "        \"Deerfield\",\n",
    "        \"Phillips Andover\",\n",
    "        \"Phillips Exeter\",\n",
    "        \"Hotchkiss\",\n",
    "        \"Lawrenceville\",\n",
    "        \"St. Paul's\",\n",
    "        \"Mt. Hermon\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "with pm.Model(coords={'school': schools}) as schools_model:\n",
    "    \n",
    "    mu = pm.Normal(\"mu\", 0, sigma=1e6)\n",
    "    tau = pm.HalfCauchy(\"tau\", 5)\n",
    "\n",
    "    theta = pm.Normal(\"theta\", mu, sigma=tau, dims='school')\n",
    "\n",
    "    obs = pm.Normal(\"obs\", theta, sigma=s, observed=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a short sample and look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with schools_model:\n",
    "    # Model fitting\n",
    "    schools_trace = pm.sample(1000, tune=1000, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running an MCMC simulation, `sample` returns an `arviz.InferenceData` object containing the samples for all the stochastic and named deterministic random variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schools_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data corresponding to each type of sampling is available as an `InferenceData` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post = schools_trace.posterior\n",
    "post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Checking\n",
    "\n",
    "The final step in Bayesian computation is model checking, in order to ensure that inferences derived from your sample are valid\n",
    "\n",
    "There are **two components** to model checking:\n",
    "\n",
    "1. Convergence diagnostics\n",
    "2. Goodness of fit\n",
    "\n",
    "Convergence diagnostics are intended to detect **lack of convergence** in the Markov chain Monte Carlo sample; it is used to ensure that you have not halted your sampling too early. However, a converged model is not guaranteed to be a good model. \n",
    "\n",
    "The second component of model checking, goodness of fit, is used to check the **internal validity** of the model, by comparing predictions from the model to the data used to fit the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence Diagnostics\n",
    "\n",
    "Valid inferences from sequences of MCMC samples are based on the\n",
    "assumption that the samples are derived from the true posterior\n",
    "distribution of interest. Theory guarantees this condition as the number\n",
    "of iterations approaches infinity. It is important, therefore, to\n",
    "determine the **minimum number of samples** required to ensure a reasonable\n",
    "approximation to the target posterior density. Unfortunately, no\n",
    "universal threshold exists across all problems, so convergence must be\n",
    "assessed independently each time MCMC estimation is performed. The\n",
    "procedures for verifying convergence are collectively known as\n",
    "*convergence diagnostics*.\n",
    "\n",
    "There are a handful of easy-to-use methods for checking convergence. Since you cannot prove convergence, but only show lack of convergence, there is no single method that is foolproof. So, its best to look at a suite of diagnostics together. \n",
    "\n",
    "We will cover the canonical set of checks:\n",
    "\n",
    "- Sampler statistics\n",
    "- Variable plotting\n",
    "- Divergences\n",
    "- R-hat\n",
    "- Effective Sample Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampler Statistics\n",
    "\n",
    "When checking for convergence or when debugging a badly behaving sampler, it is often helpful to take a closer look at what the sampler is doing. For this purpose some samplers export statistics for each generated sample.\n",
    "\n",
    "NUTS provides several metrics related to the performance of the sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schools_trace.sample_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample statistics variables are defined as follows:\n",
    "\n",
    "- `process_time_diff`: The time it took to draw the sample, as defined by the python standard library time.process_time. This counts all the CPU time, including worker processes in BLAS and OpenMP.\n",
    "\n",
    "- `step_size`: The current integration step size.\n",
    "\n",
    "- `diverging`: (boolean) Indicates the presence of leapfrog transitions with large energy deviation from starting and subsequent termination of the trajectory. \"large\" is defined as `max_energy_error` going over a threshold.\n",
    "\n",
    "- `lp`: The joint log posterior density for the model (up to an additive constant).\n",
    "\n",
    "- `energy`: The value of the Hamiltonian energy for the accepted proposal (up to an additive constant).\n",
    "\n",
    "- `energy_error`: The difference in the Hamiltonian energy between the initial point and the accepted proposal.\n",
    "\n",
    "- `perf_counter_diff`: The time it took to draw the sample, as defined by the python standard library time.perf_counter (wall time).\n",
    "\n",
    "- `perf_counter_start`: The value of time.perf_counter at the beginning of the computation of the draw.\n",
    "\n",
    "- `n_steps`: The number of leapfrog steps computed. It is related to `tree_depth` with `n_steps <= 2^tree_dept`.\n",
    "\n",
    "- `max_energy_error`: The maximum absolute difference in Hamiltonian energy between the initial point and all possible samples in the proposed tree.\n",
    "\n",
    "- `acceptance_rate`: The average acceptance probabilities of all possible samples in the proposed tree.\n",
    "\n",
    "- `step_size_bar`: The current best known step-size. After the tuning samples, the step size is set to this value. This should converge during tuning.\n",
    "\n",
    "- `tree_depth`: The number of tree doublings in the balanced binary tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be helpful to plot some of these variables, rather than staring at vectors of numbers!\n",
    "\n",
    "#### Tree Depth\n",
    "\n",
    "In the No-U-Turn Sampler (NUTS), each proposal constructs a balanced binary tree\n",
    "of candidate states by recursively doubling the number of leapfrog steps.\n",
    "\n",
    "- At tree depth *d*, the sampler can take up to `2**d` leapfrog steps.\n",
    "- Doubling stops when:\n",
    "  - A U-turn condition is detected (the trajectory reverses), or\n",
    "  - A maximum tree depth limit (`max_tree_depth`) is reached.\n",
    "\n",
    "Tree depth controls the maximum path length of each proposal:\n",
    "- Higher depths allow the sampler to explore further along the posterior\n",
    "  geometry.\n",
    "- Consistently hitting the maximum depth often indicates tuning or geometry issues,\n",
    "  such as a step size that is too small.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schools_trace.sample_stats[\"tree_depth\"].plot(col=\"chain\", ls=\"none\", marker=\".\", alpha=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see if the acceptance rate is close to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schools_trace.sample_stats[\"acceptance_rate\"].plot.hist(bins=20, density=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that NUTS generates a binary tree of samples, so here the acceptance rate is the average of the acceptance probabilities of all the samples in the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Visualization with ArviZ\n",
    "\n",
    "[ArviZ](https://arviz-devs.github.io/arviz/) is a Python package for exploratory analysis of Bayesian models. It includes functions for posterior analysis, model checking, comparison and diagnostics and is desingefd to work with a range of Bayesian inference libraries (not just PyMC).\n",
    "\n",
    "ArviZ is built on top of the popular libraries xarray and matplotlib. It is also built with the same design principles as PyMC, so if you are familiar with PyMC, you will find ArviZ easy to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traceplot \n",
    "\n",
    "Perhaps the most-used ArviZ plot is the traceplot, obtained via the `plot_trace` function. This is a simple plot that is a good quick check to make sure nothing is obviously wrong, and is usually the first diagnostic step you will take. You've seen these already: just the time series of samples for an individual variable.\n",
    "\n",
    "The `plot_trace` function from ArViZ by default generates a kernel density plot and a trace plot, with a different color for each chain of the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(schools_trace, var_names=['mu', 'tau']);\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample is deliberately inadequate. Looking at the trace plot, the problems should be apparent.\n",
    "\n",
    "Can you identify the issues, based on what you learned in the previous section?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divergences\n",
    "\n",
    "As we have seen, Hamiltonian Monte Carlo (and NUTS) performs numerical integration in order to explore the posterior distribution of a model. When the integration goes wrong, it can go dramatically wrong. \n",
    "\n",
    "For example, here are some Hamiltonian trajectories on the distribution of two correlated variables. Can you spot the divergent path?\n",
    "\n",
    "![divering HMC](images/diverging_hmc.png)\n",
    "\n",
    "The reason that this happens is that there may be parts of the posterior which are **hard to explore** for geometric reasons. Two ways of solving divergences are\n",
    "\n",
    "1. **Set a higher \"target accept\" rate**: Similarly (but not the same) as for Metropolis-Hastings, larger integrator steps lead to lower acceptance rates. A higher `target_accept` will generally cause a smaller step size, and more accurate integration.\n",
    "2. **Reparametrize**: If you can write your model in a different way that has the same joint probability density, you might do thpt. A lot of work is being done to automate this, since it requires careful work, and one goal of a probabilistic programming language is to iterate quickly. See [Hoffmann, Johnson, Tran (2018)](https://arxiv.org/abs/1811.11926), [Gorinova, Moore, Hoffmann (2019)](https://arxiv.org/abs/1906.03028).\n",
    "\n",
    "You should be wary of a trace that contains many divergences (particularly those clustered in particular regions of the parameter space), and give thought to how to fix them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divergence example\n",
    "\n",
    "The trajectories above are from a famous example of a difficult geometry: Neal's funnel. It is problematic because the geometry is very different in some regions of the state space relative to others. Specifically, for hierarchical models, as the scale parameter changes in size so do the values of the parameters it is constraining. When the variance is close to zero, the parameter space is very constrained relative to the majority of the support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neals_funnel(dims=2):\n",
    "    with pm.Model() as funnel:\n",
    "        v = pm.Normal('v', 0, 3)\n",
    "        x_vec = pm.MvNormal('x_vec', mu=pt.zeros(dims), cov=2 * pt.exp(v) * pt.eye(dims), shape=dims)\n",
    "    return funnel\n",
    "\n",
    "with neals_funnel():\n",
    "    funnel_trace = pm.sample(random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyMC provides us feedback on divergences, including a count and a recommendation on how to address them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diverging_ind = funnel_trace.sample_stats['diverging'].values[0].nonzero()\n",
    "diverging_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = az.plot_pair(funnel_trace)\n",
    "ax[0][0].plot(funnel_trace.posterior['v'].sel(chain=0).values[diverging_ind], \n",
    "              funnel_trace.posterior['x_vec'].sel(chain=0).values[diverging_ind].squeeze(), 'y.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the divergent samples are clustered toward the narrow part of the funnel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Scale Reduction: $\\hat{R}$\n",
    "\n",
    "Roughly, $\\hat{R}$ (*R-Hat*, or the *Gelman-Rubin statistic*) is the ratio of between-chain variance to within-chain variance. This diagnostic uses multiple chains to\n",
    "check for lack of convergence, and is based on the notion that if\n",
    "multiple chains have converged, by definition they should appear very\n",
    "similar to one another; if not, one or more of the chains has failed to\n",
    "converge.\n",
    "\n",
    "$\\hat{R}$ uses an analysis of variance approach to\n",
    "assessing convergence. That is, it calculates both the between-chain\n",
    "variance (B) and within-chain variance (W), and assesses whether they\n",
    "are different enough to worry about convergence. Assuming $m$ chains,\n",
    "each of length $n$, quantities are calculated by:\n",
    "\n",
    "$$\\begin{align}B &= \\frac{n}{m-1} \\sum_{j=1}^m (\\bar{\\theta}_{.j} - \\bar{\\theta}_{..})^2 \\\\\n",
    "W &= \\frac{1}{m} \\sum_{j=1}^m \\left[ \\frac{1}{n-1} \\sum_{i=1}^n (\\theta_{ij} - \\bar{\\theta}_{.j})^2 \\right]\n",
    "\\end{align}$$\n",
    "\n",
    "for each scalar estimand $\\theta$. Using these values, an estimate of\n",
    "the marginal posterior variance of $\\theta$ can be calculated:\n",
    "\n",
    "$$\\hat{\\text{Var}}(\\theta | y) = \\frac{n-1}{n} W + \\frac{1}{n} B$$\n",
    "\n",
    "Assuming $\\theta$ was initialized to arbitrary starting points in each\n",
    "chain, this quantity will overestimate the true marginal posterior\n",
    "variance. At the same time, $W$ will tend to underestimate the\n",
    "within-chain variance early in the sampling run. However, in the limit\n",
    "as $n \\rightarrow \n",
    "\\infty$, both quantities will converge to the true variance of $\\theta$.\n",
    "In light of this, $\\hat{R}$ monitors convergence using\n",
    "the ratio:\n",
    "\n",
    "$$\\hat{R} = \\sqrt{\\frac{\\hat{\\text{Var}}(\\theta | y)}{W}}$$\n",
    "\n",
    "This is called the **potential scale reduction**, since it is an estimate of\n",
    "the potential reduction in the scale of $\\theta$ as the number of\n",
    "simulations tends to infinity. In practice, we look for values of\n",
    "$\\hat{R}$ close to one (say, less than 1.1) to be confident that a\n",
    "particular estimand has converged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(schools_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effective Sample Size\n",
    "\n",
    "In general, samples drawn from MCMC algorithms will be autocorrelated. Unless the autocorrelation is very severe, this is not a big deal, other than the fact that autocorrelated chains may require longer sampling in order to adequately characterize posterior quantities of interest. The calculation of autocorrelation is performed for each lag $i=1,2,\\ldots,k$ (the correlation at lag 0 is, of course, 1) by: \n",
    "\n",
    "$$\\hat{\\rho}_i = 1 - \\frac{V_i}{2\\hat{\\text{Var}}(\\theta | y)}$$\n",
    "\n",
    "where $\\hat{\\text{Var}}(\\theta | y)$ is the same estimated variance as calculated for the Gelman-Rubin statistic, and $V_i$ is the variogram at lag $i$ for $\\theta$:\n",
    "\n",
    "$$\\text{V}_i = \\frac{1}{m(n-i)}\\sum_{j=1}^m \\sum_{k=i+1}^n (\\theta_{jk} - \\theta_{j(k-i)})^2$$\n",
    "\n",
    "This autocorrelation can be visualized using the `plot_autocorr` function in ArviZ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_autocorr(schools_trace, var_names=['mu', 'tau'], combined=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see very severe autocorrelation in both variables, which is not surprising given the trace that we observed earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amount of correlation in an MCMC sample influences the **effective sample size** (ESS) of the sample. The ESS estimates how many *independent* draws contain the same amount of information as the *dependent* sample obtained by MCMC sampling.\n",
    "\n",
    "Given a series of samples $x_j$, the empirical mean is\n",
    "\n",
    "$$\n",
    "\\hat{\\mu} = \\frac{1}{n}\\sum_{j=1}^n x_j\n",
    "$$\n",
    "\n",
    "and the variance of the estimate of the empirical mean is \n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{\\mu}) = \\frac{\\sigma^2}{n},\n",
    "$$\n",
    "where $\\sigma^2$ is the true variance of the underlying distribution.\n",
    "\n",
    "Then the effective sample size is defined as the denominator that makes this relationship still be true:\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{\\mu}) = \\frac{\\sigma^2}{n_{\\text{eff}}}.\n",
    "$$\n",
    "\n",
    "The effective sample size is estimated using the partial sum:\n",
    "\n",
    "$$\\hat{n}_{eff} = \\frac{n}{1 + 2\\sum_{i=1}^T \\hat{\\rho}_i}$$\n",
    "\n",
    "where $T$ is the first odd integer such that $\\hat{\\rho}_{T+1} + \\hat{\\rho}_{T+2}$ is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_ess(schools_trace, var_names=['tau'])\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ArViZ, we can visualize the evolution of ESS as the MCMC sample accumulates. When the model is converging properly, both lines in this plot should be approximately linear.\n",
    "\n",
    "The standard ESS estimate, which mainly assesses how well the centre of the distribution is resolved, is referred to as **bulk-ESS**. In order to estimate intervals reliably, it is also important to consider the **tail-ESS**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_ess(schools_trace, var_names=['mu'], kind='evolution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Fraction of Missing Information\n",
    "\n",
    "The Bayesian fraction of missing information (BFMI) is a measure of how hard it is to\n",
    "sample level sets of the posterior at each iteration. Specifically, it quantifies **how well momentum resampling matches the marginal energy distribution**. \n",
    "\n",
    "$$\\text{BFMI} = \\frac{\\mathbb{E}_{\\pi}[\\text{Var}_{\\pi_{E|q}}(E|q)]}{\\text{Var}_{\\pi_{E}}(E)}$$\n",
    "\n",
    "$$\\widehat{\\text{BFMI}} = \\frac{\\sum_{i=1}^N (E_n - E_{n-1})^2}{\\sum_{i=1}^N (E_n - \\bar{E})^2}$$\n",
    "\n",
    "BFMI is essentially a measure of the association between the energy of a state and the energy of the next state, or more precisely, it compares the average squared change in energy between successive samples to the overall variance of the energy across all samples. The \"missing information\" refers to the information that the sampler fails to gain about the posterior because it cannot efficiently traverse the energy landscape.\n",
    "\n",
    "A small value indicates that the adaptation phase of the sampler was unsuccessful, and invoking the central limit theorem may not be valid. It indicates whether the sampler is able to *efficiently* explore the posterior distribution.\n",
    "\n",
    "Though there is not an established rule of thumb for an adequate threshold, values close to one are optimal. Reparameterizing the model is sometimes helpful for improving this statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BFMI calculation is only available in samples that were simulated using HMC or NUTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.bfmi(schools_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of diagnosting this phenomenon is by comparing the overall distribution of \n",
    "energy levels with the *change* of energy between successive samples. Ideally, they should be very similar.\n",
    "\n",
    "If the distribution of energy transitions is narrow relative to the marginal energy distribution, this is a sign of inefficient sampling, as many transitions are required to completely explore the posterior. On the other hand, if the energy transition distribution is similar to that of the marginal energy, this is evidence of efficient sampling, resulting in near-independent samples from the posterior.\n",
    "\n",
    "As an example, if we look at the energy plot of our eight schools model, the low BFMI values (which result in poor overlap in the energy distributions) suggest taht the sampler is having trouble exploring different energy levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_energy(schools_trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can attempt to improve the efficiency of the sampler by reparameterizing the random effect to be non-centered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([28, 8, -3, 7, -1, 1, 18, 12])\n",
    "s = np.array([15, 10, 16, 11, 9, 11, 10, 18])\n",
    "schools = np.array(\n",
    "    [\n",
    "        \"Choate\",\n",
    "        \"Deerfield\",\n",
    "        \"Phillips Andover\",\n",
    "        \"Phillips Exeter\",\n",
    "        \"Hotchkiss\",\n",
    "        \"Lawrenceville\",\n",
    "        \"St. Paul's\",\n",
    "        \"Mt. Hermon\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "with pm.Model(coords={'school': schools}) as schools_uncentered:\n",
    "    \n",
    "    mu = pm.Normal(\"mu\", 0, sigma=1e6)\n",
    "    tau = pm.HalfCauchy(\"tau\", 5)\n",
    "\n",
    "    z = pm.Normal('z', dims='school')\n",
    "    theta = pm.Deterministic(\"theta\", mu + tau*z, dims='school')\n",
    "\n",
    "    obs = pm.Normal(\"obs\", theta, sigma=s, observed=y)\n",
    "\n",
    "    trace_uncentered = pm.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_energy(trace_uncentered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_uncentered, var_names=['mu', 'tau']);\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goodness of Fit\n",
    "\n",
    "As noted at the beginning of this section, convergence diagnostics are only the first step in the evaluation\n",
    "of MCMC model outputs. It is possible for an entirely unsuitable model to converge, so additional steps are needed to ensure that the estimated model adequately fits the data. \n",
    "\n",
    "One intuitive way of evaluating model fit is to compare model predictions with the observations used to fit\n",
    "the model. In other words, the fitted model can be used to simulate data, and the distribution of the simulated data should resemble the distribution of the actual data.\n",
    "\n",
    "Fortunately, simulating data from the model is a natural component of the Bayesian modelling framework. Recall, from the discussion on prediction, the posterior predictive distribution:\n",
    "\n",
    "$$p(\\tilde{y}|y) = \\int p(\\tilde{y}|\\theta) f(\\theta|y) d\\theta$$\n",
    "\n",
    "Here, $\\tilde{y}$ represents some hypothetical new data that would be expected, taking into account the posterior uncertainty in the model parameters. \n",
    "\n",
    "Sampling from the posterior predictive distribution is easy in PyMC. The `sample_posterior_predictive` function draws posterior predictive samples from all of the observed variables in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with schools_uncentered:\n",
    "    pm.sample_posterior_predictive(trace_uncentered, extend_inferencedata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The degree to which simulated data correspond to observations can be evaluated visually. This allows for a qualitative comparison of model-based replicates and observations. If there is poor fit, the true value of the data may appear in the tails of the histogram of replicated data, while a good fit will tend to show the true data in high-probability regions of the posterior predictive distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_ppc(trace_uncentered);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_ppc(trace_uncentered, kind='cumulative', mean=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the predictive performance of our model by examining the residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = trace_uncentered.posterior_predictive[\"obs\"].mean([\"chain\", \"draw\"]).values\n",
    "residuals = y - y_pred\n",
    "\n",
    "go.Figure().add_trace(\n",
    "    go.Scatter(\n",
    "        x=y_pred,\n",
    "        y=residuals,\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            color='royalblue',\n",
    "            size=8,\n",
    "            opacity=0.6\n",
    "        ),\n",
    "        name='Residuals'\n",
    "    )\n",
    ").add_shape(\n",
    "    type=\"line\",\n",
    "    x0=float(y_pred.min()),\n",
    "    x1=float(y_pred.max()),\n",
    "    y0=0,\n",
    "    y1=0,\n",
    "    line=dict(\n",
    "        color=\"red\",\n",
    "        width=2\n",
    "    )\n",
    ").update_layout(\n",
    "    title='Residual Plot',\n",
    "    xaxis_title='Predicted values',\n",
    "    yaxis_title='Residuals',\n",
    "    width=600,\n",
    "    plot_bgcolor='white',\n",
    "    showlegend=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Workflow\n",
    "\n",
    "Strengths of Bayesian statistics that are critical:\n",
    "* Great flexibility to quickly and iteratively build statistical models\n",
    "* Offers principled way of dealing with uncertainty\n",
    "* Don't just want most likely outcome but distribution of all possible outcomes\n",
    "* Allows expert information to guide model by using informative priors\n",
    "\n",
    "The Bayesian workflow consists of:\n",
    "* How to go from data to a model idea\n",
    "* How to find priors for your model\n",
    "* How to evaluate a model\n",
    "* How to iteratively improve a model\n",
    "* How to forecast into the future\n",
    "* How powerful generative modeling can be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COVID-19 Case Study\n",
    "\n",
    "Let's apply the Bayesian workflow to a real-world problem: modeling COVID-19 cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_covid_data\n",
    "\n",
    "df = load_covid_data.load_data(drop_states=True, filter_n_days_100=2)\n",
    "countries = df.country.unique()\n",
    "n_countries = len(countries)\n",
    "df = df.loc[lambda x: (x.days_since_100 >= 0)]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Workflow\n",
    "\n",
    "A good workflow to adopt when developing models is:\n",
    "\n",
    "1. Plot the data\n",
    "2. Build model\n",
    "3. Run prior predictive check\n",
    "4. Fit model\n",
    "5. Assess convergence\n",
    "6. Run posterior predictive check\n",
    "7. Improve model\n",
    "\n",
    "#### 1. Plot the data\n",
    "\n",
    "We will look at German COVID-19 cases. At first, we will only look at the first 30 days after Germany crossed 100 cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "country = 'Germany'\n",
    "date = '2020-07-31'\n",
    "df_country = df.query(f'country==\"{country}\"').loc[:date].iloc[:30]\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add confirmed cases line\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df_country.index,\n",
    "        y=df_country.confirmed,\n",
    "        mode='lines+markers',\n",
    "        name='Confirmed cases',\n",
    "        line=dict(color='royalblue', width=2),\n",
    "        marker=dict(size=6)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=dict(\n",
    "        text=f'COVID-19 Cases in {country}',\n",
    "        x=0.5\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title='Date',\n",
    "        tickformat='%b %d\\n%Y'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Confirmed cases',\n",
    "        gridcolor='lightgray'\n",
    "    ),\n",
    "    width=1000,\n",
    "    height=800,\n",
    "    plot_bgcolor='white'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Build an initial model\n",
    "\n",
    "The above line looks exponential. This matches with knowledge from epidemiology, where early in an epidemic it grows exponentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get time-range of days since 100 cases were crossed\n",
    "t = df_country.days_since_100.values\n",
    "# Get number of confirmed cases for Germany\n",
    "confirmed = df_country.confirmed.values\n",
    "\n",
    "with pm.Model() as model_exp1:\n",
    "    # Intercept\n",
    "    a = pm.Normal('a', mu=0, sigma=100)\n",
    "\n",
    "    # Slope\n",
    "    b = pm.Normal('b', mu=0.3, sigma=0.3)\n",
    "\n",
    "    # Exponential regression\n",
    "    growth = a * (1 + b) ** t\n",
    "\n",
    "    # Error term\n",
    "    eps = pm.HalfNormal('eps', 100)\n",
    "\n",
    "    # Likelihood\n",
    "    pm.Normal('obs',\n",
    "              mu=growth,\n",
    "              sigma=eps,\n",
    "              observed=confirmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Run prior predictive check\n",
    "\n",
    "Without even fitting the model to our data, we generate new potential data from our priors. Usually we have less intuition about the parameter space, where we define our priors, and more intution about what data we might expect to see. A prior predictive check thus allows us to make sure the model can generate the types of data we expect to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_exp1:\n",
    "    prior_pred = pm.sample_prior_predictive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_samples = prior_pred.prior_predictive[\"obs\"].values.squeeze()\n",
    "\n",
    "def plot_prior_predictive(obs_samples, y_range=None, x_range=None, \n",
    "                         title=\"Prior predictive\", x_label=\"Days since 100 cases\", \n",
    "                         y_label=\"Positive cases\", log_y=False):\n",
    "    num_samples, num_timesteps = obs_samples.shape\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    for i in range(num_samples):\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=np.arange(num_timesteps),\n",
    "            y=obs_samples[i],\n",
    "            mode='lines',\n",
    "            line=dict(color='rgba(128,128,128,0.1)'),\n",
    "            showlegend=False\n",
    "        ))\n",
    "    fig.update_layout(\n",
    "        yaxis_range=y_range,\n",
    "        xaxis_range=x_range,\n",
    "        title=title,\n",
    "        xaxis_title=x_label,\n",
    "        yaxis_title=y_label,\n",
    "        template=\"plotly_white\",\n",
    "        yaxis_type=\"log\" if log_y else \"linear\"\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prior_predictive(obs_samples=obs_samples, y_range=(-1000, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What's wrong with this model?\n",
    "\n",
    "There are several issues with this model:\n",
    "1. Cases can't be negative\n",
    "2. Cases can not start at 0, as we set it to start at above 100\n",
    "3. Case counts can't go down\n",
    "\n",
    "Let's improve our model. The presence of negative cases is due to us using a Normal likelihood. Instead, let's use a `NegativeBinomial`, which is similar to `Poisson` which is commonly used for count-data but has an extra dispersion parameter that allows more flexiblity in modeling the variance of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df_country.days_since_100.values\n",
    "confirmed = df_country.confirmed.values\n",
    "\n",
    "with pm.Model() as model_exp2:\n",
    "    # Intercept\n",
    "    a = pm.Normal('a', mu=100, sigma=25)\n",
    "\n",
    "    # Slope\n",
    "    b = pm.Normal('b', mu=0.3, sigma=0.1)\n",
    "\n",
    "    # Exponential regression\n",
    "    growth = a * (1 + b) ** t\n",
    "\n",
    "    # Likelihood\n",
    "    pm.NegativeBinomial('obs',\n",
    "                 growth,\n",
    "                 alpha=pm.Gamma(\"alpha\", mu=6, sigma=1),\n",
    "                 observed=confirmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_exp2:\n",
    "    prior_pred = pm.sample_prior_predictive()\n",
    "\n",
    "plot_prior_predictive(obs_samples=prior_pred.prior_predictive['obs'].values.squeeze(), y_range=(-100, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_exp2:\n",
    "    trace_exp2 = pm.sample(chains=4, cores=4, tune=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Assess convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_exp2, var_names=['a', 'b', 'alpha'])\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(trace_exp2, var_names=['a', 'b', 'alpha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_energy(trace_exp2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Run posterior predictive check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_exp2:\n",
    "    # Draw samples from posterior predictive\n",
    "    post_pred = pm.sample_posterior_predictive(trace_exp2.posterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "posterior_samples = post_pred.posterior_predictive['obs'].sel(chain=0).values.squeeze()\n",
    "\n",
    "def plot_posterior_predictive(posterior_samples, confirmed):\n",
    "    fig = go.Figure()\n",
    "    for i in range(posterior_samples.shape[0]):\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=list(range(posterior_samples.shape[1])),\n",
    "            y=posterior_samples[i],\n",
    "            mode='lines',\n",
    "            line=dict(color='rgba(128,128,128,0.05)', width=1),\n",
    "            showlegend=False,\n",
    "            hoverinfo='skip'\n",
    "        ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(range(len(confirmed))),\n",
    "        y=confirmed,\n",
    "        mode='lines',\n",
    "        line=dict(color='red', width=2),\n",
    "        name='data'\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=country,\n",
    "        xaxis_title=\"Days since 100 cases\",\n",
    "        yaxis_title=\"Confirmed cases (log scale)\",\n",
    "        yaxis_type=\"log\",\n",
    "        height=800,\n",
    "        width=1000,\n",
    "        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.01)\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "plot_posterior_predictive(posterior_samples, confirmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = post_pred.posterior_predictive[\"obs\"].sel(chain=0).values - confirmed\n",
    "fig = go.Figure()\n",
    "for i in range(resid.shape[0]):\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(range(resid.shape[1])),\n",
    "        y=resid[i],\n",
    "        mode='lines',\n",
    "        line=dict(color='rgba(128,128,128,0.01)', width=1),\n",
    "        showlegend=False,\n",
    "        hoverinfo='skip'\n",
    "    ))\n",
    "fig.update_layout(\n",
    "    yaxis=dict(range=[-50_000, 200_000]),\n",
    "    yaxis_title=\"Residual\",\n",
    "    xaxis_title=\"Days since 100 cases\",\n",
    "    height=800,\n",
    "    width=1000\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Improve model - Logistic Growth Model\n",
    "\n",
    "The exponential model doesn't capture the plateau in cases that we expect to see over time. Let's implement a logistic growth model which has an S-shaped curve that better represents epidemic dynamics with a carrying capacity.\n",
    "\n",
    "![Logistic Growth](images/logistic_growth.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country = df.query(f'country==\"{country}\"').loc[:date]\n",
    "\n",
    "with pm.Model() as logistic_model:\n",
    "    t_data = pm.Data('t', df_country.days_since_100.values)\n",
    "    confirmed_data = pm.Data('confirmed', df_country.confirmed.values)\n",
    "\n",
    "    # Intercept\n",
    "    a0 = pm.HalfNormal('a0', sigma=25)\n",
    "    intercept = pm.Deterministic('intercept', a0 + 100)\n",
    "\n",
    "    # Slope\n",
    "    b = pm.HalfNormal('b', sigma=0.2)\n",
    "    \n",
    "    carrying_capacity = pm.Uniform('carrying_capacity',\n",
    "                                   lower=1_000,\n",
    "                                   upper=80_000_000)\n",
    "    # Transform carrying_capacity to a\n",
    "    a = carrying_capacity / intercept - 1\n",
    "\n",
    "    # Logistic\n",
    "    growth = carrying_capacity / (1 + a * pm.math.exp(-b * t_data))\n",
    "\n",
    "    # Likelihood\n",
    "    pm.NegativeBinomial('obs',\n",
    "                 growth,\n",
    "                 alpha=pm.Gamma(\"alpha\", mu=6, sigma=1),\n",
    "                 observed=confirmed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with logistic_model:\n",
    "    prior_pred = pm.sample_prior_predictive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prior_predictive(prior_pred.prior_predictive['obs'].squeeze().values, log_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with logistic_model:\n",
    "    # Inference\n",
    "    trace_logistic = pm.sample(chains=4, cores=4, tune=2000, target_accept=0.9)\n",
    "    \n",
    "    # Sample posterior predcitive\n",
    "    pm.sample_posterior_predictive(trace_logistic, extend_inferencedata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_logistic)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_posterior_predictive(trace_logistic.posterior_predictive['obs'].sel(chain=0).squeeze().values, confirmed=df_country.confirmed.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting\n",
    "\n",
    "One of the key strengths of Bayesian modeling is the ability to make predictions with uncertainty. Let's extend our prediction window to forecast future cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a forecast window\n",
    "forecast_days = 60  # 2 months forecast\n",
    "future_days = np.arange(len(df_country.days_since_100.values), \n",
    "                        len(df_country.days_since_100.values) + forecast_days)\n",
    "\n",
    "with logistic_model:\n",
    "    # Update our data containers for forecasting\n",
    "    pm.set_data({'t': np.concatenate([df_country.days_since_100.values, future_days]),\n",
    "                 'confirmed': np.concatenate([df_country.confirmed.values, \n",
    "                                            np.zeros(forecast_days, dtype='int')])})\n",
    "\n",
    "    forecast = pm.sample_posterior_predictive(trace_logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "historical_days = len(df_country.days_since_100.values)\n",
    "all_days = np.arange(historical_days + forecast_days)\n",
    "\n",
    "forecast_samples = forecast.posterior_predictive['obs'].values\n",
    "forecast_mean = forecast_samples.mean(axis=(0, 1))\n",
    "forecast_lower = np.percentile(forecast_samples, 2.5, axis=(0, 1))\n",
    "forecast_upper = np.percentile(forecast_samples, 97.5, axis=(0, 1))\n",
    "\n",
    "go.Figure().add_trace(\n",
    "    go.Scatter(\n",
    "        x=np.arange(historical_days),\n",
    "        y=df_country.confirmed.values,\n",
    "        mode='lines+markers',\n",
    "        name='Observed cases',\n",
    "        line=dict(color='black', width=2),\n",
    "        marker=dict(size=6)\n",
    "    )\n",
    ").add_trace(\n",
    "    go.Scatter(\n",
    "        x=all_days,\n",
    "        y=forecast_mean,\n",
    "        mode='lines',\n",
    "        name='Mean forecast',\n",
    "        line=dict(color='blue', width=2)\n",
    "    )\n",
    ").add_trace(\n",
    "    go.Scatter(\n",
    "        x=np.concatenate([all_days, all_days[::-1]]),\n",
    "        y=np.concatenate([forecast_upper, forecast_lower[::-1]]),\n",
    "        fill='toself',\n",
    "        fillcolor='rgba(0, 0, 255, 0.2)',\n",
    "        line=dict(color='rgba(255, 255, 255, 0)'),\n",
    "        name='95% credible interval'\n",
    "    )\n",
    ").add_shape(\n",
    "    type=\"line\",\n",
    "    x0=historical_days-1, x1=historical_days-1,\n",
    "    y0=0, y1=forecast_mean[historical_days-1],\n",
    "    line=dict(color='gray', width=2, dash='dash')\n",
    ").update_layout(\n",
    "    title=f'COVID-19 Cases Forecast for {country}',\n",
    "    xaxis=dict(\n",
    "        title='Days since 100 cases'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Confirmed cases'\n",
    "    ),\n",
    "    width=1200,\n",
    "    height=800,\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.02,\n",
    "        xanchor=\"right\",\n",
    "        x=1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "Gelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (2003). Bayesian Data Analysis, Second Edition (Chapman & Hall/CRC Texts in Statistical Science) (2nd ed.). Chapman and Hall/CRC.\n",
    "\n",
    "Gelman, A., & Rubin, D. B. (1992). Inference from iterative simulation using multiple sequences. Statistical Science. A Review Journal of the Institute of Mathematical Statistics, 457472.\n",
    "\n",
    "Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Brkner, P.-C. (2019). Rank-normalization, folding, and localization: An improved $\\hat{R}$ for assessing convergence of MCMC. arXiv preprint arXiv:1903.08008.\n",
    "\n",
    "Gelman, A., Hwang, J., & Vehtari, A. (2014). Understanding predictive information criteria for Bayesian models. Statistics and Computing, 24(6), 9971016.\n",
    "\n",
    "Betancourt, M. (2016). Diagnosing Suboptimal Cotangent Disintegrations in Hamiltonian Monte Carlo. arXiv preprint arXiv:1604.00695.\n",
    "\n",
    "Gabry, J., Simpson, D., Vehtari, A., Betancourt, M., & Gelman, A. (2019). Visualization in Bayesian workflow. Journal of the Royal Statistical Society: Series A (Statistics in Society), 182(2), 389-402."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
