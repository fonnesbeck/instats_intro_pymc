{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 1: Introduction to Probabilistic Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import platform\n",
    "import polars as pl\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy import stats\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 75\n",
    "\n",
    "if platform.system() == 'Linux':\n",
    "    # Multiprocessing behaves oddly on Linux, so we need to set the start method\n",
    "    import multiprocessing as mp\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "\n",
    "RANDOM_SEED = 20090425"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of conducting Bayesian inference can be broken down into three general steps (Gelman *et al.* 2013):\n",
    "\n",
    "![](images/123.png)\n",
    "\n",
    "### Step 1: Specify a probability model\n",
    "\n",
    "Bayesian statistics involves using probability models to solve problems. So, the first task is to *completely specify* the model in terms of probability distributions. This includes everything: unknown parameters, data, covariates, missing data, predictions. All must be assigned some probability density.\n",
    "\n",
    "This step involves making choices.\n",
    "\n",
    "- what is the form of the sampling distribution of the data?\n",
    "- what form best describes our uncertainty in the unknown parameters?\n",
    "\n",
    "### Step 2: Calculate a posterior distribution\n",
    "\n",
    "The mathematical form $p(\\theta | y)$ that we associated with the Bayesian approach is referred to as a **posterior distribution**.\n",
    "\n",
    "> posterior /pos·ter·i·or/ (pos-tēr´e-er) later in time; subsequent.\n",
    "\n",
    "Why posterior? Because it tells us what we know about the unknown $\\theta$ *after* having observed $y$.\n",
    "\n",
    "This posterior distribution is formulated as a function of the probability model that was specified in Step 1. Usually, we can write it down but we cannot calculate it analytically. In fact, the difficulty inherent in calculating the posterior distribution for most models of interest is perhaps the major contributing factor for the lack of widespread adoption of Bayesian methods for data analysis. Various strategies for doing so comprise this tutorial.\n",
    "\n",
    "**But**, once the posterior distribution is calculated, you get a lot for free:\n",
    "\n",
    "- point estimates\n",
    "- credible intervals\n",
    "- quantiles\n",
    "- predictions\n",
    "\n",
    "### Step 3: Check your model\n",
    "\n",
    "Though frequently ignored in practice, it is critical that the model and its outputs be assessed before using the outputs for inference. Models are specified based on assumptions that are largely unverifiable, so the least we can do is examine the output in detail, relative to the specified model and the data that were used to fit the model.\n",
    "\n",
    "Specifically, we must ask:\n",
    "\n",
    "- does the model fit data?\n",
    "- are the conclusions reasonable?\n",
    "- are the outputs sensitive to changes in model structure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: binomial calculation\n",
    "\n",
    "Consider estimating a population sex ratio based on the sex of a sample of newborn babies in a hospital: suppose we have observed **3 female children from a sample of 5 births**.\n",
    "\n",
    "How would we specify a Bayesian model for this problem?\n",
    "\n",
    "- What is the likelihood?\n",
    "- What is/are the prior(s)?\n",
    "\n",
    "**Binomial model** is suitable for data that are generated from a sequence of exchangeable Bernoulli trials. These data can be summarized by $y$, the number of times the event of interest occurs, and $n$, the total number of trials. The model parameter is the expected proportion of trials that an event occurs.\n",
    "\n",
    "$$p(Y|\\theta) = \\frac{n!}{y! (n-y)!} \\theta^{y} (1-\\theta)^{n-y}$$\n",
    "\n",
    "where $y \\in \\{0, 1, \\ldots, n\\}$ and $\\theta \\in [0, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 11)\n",
    "fig = go.Figure()\n",
    "\n",
    "bar_width = 0.2\n",
    "ps = [0.2, 0.5, 0.8]\n",
    "colors = [\"#636EFA\", \"#EF553B\", \"#00CC96\"] \n",
    "\n",
    "for i, (n, p) in enumerate(zip([10, 10, 10], ps)):\n",
    "    dist = pm.Binomial.dist(n=n, p=p)\n",
    "    pmf = np.exp(pm.logp(dist, x).eval())\n",
    "    offset = (i - 1) * bar_width \n",
    "    fig.add_trace(go.Bar(\n",
    "        x=x + offset,\n",
    "        y=pmf,\n",
    "        width=bar_width,\n",
    "        name=f\"Binom(n={n}, p={p})\",\n",
    "        opacity=0.7,\n",
    "        marker_color=colors[i]\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Binomial Distribution\",\n",
    "    xaxis_title=\"Number of successes\",\n",
    "    yaxis_title=\"Probability\",\n",
    "    barmode=\"overlay\",\n",
    "    width=600,\n",
    ")\n",
    "fig.update_xaxes(\n",
    "    tickmode=\"array\",\n",
    "    tickvals=x,\n",
    "    ticktext=[str(val) for val in x]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we require the specification of a prior distribution for the binomial probability $\\theta$. One reasonable choice is a **uniform** prior on [0,1] which has two implications:\n",
    "\n",
    "1. makes all probability values equally probable *a priori* \n",
    "2. makes calculation of the posterior easy\n",
    "\n",
    "The second task in performing Bayesian inference is, given a fully-specified model, to calculate a posterior distribution. As we have specified the model, we can calculate a posterior distribution up to a proportionality constant (that is, a probability distribution that is **unnormalized**):\n",
    "\n",
    "$$P(\\theta | n, y) \\propto P(y | n, \\theta) P(\\theta) = \\theta^y (1-\\theta)^{n-y}$$\n",
    "\n",
    "We can present *different posterior distributions* as a function of *different realized data*.\n",
    "\n",
    "We can also calculate posterior estimates for $\\theta$ by *maximizing* the unnormalized posterior using optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_range = np.linspace(0, 1, num=100, endpoint=False)\n",
    "\n",
    "def binomial_posterior(n, y):\n",
    "    posterior_values = (theta_range**y) * (1 - theta_range)**(n-y)\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=theta_range, y=posterior_values, mode='lines', name=f'n={n}, y={y}'))\n",
    "    fig.update_layout(title=f'Binomial Posterior (n={n}, y={y})',\n",
    "                      xaxis_title='θ', width=600)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "binomial_posterior(n=5, y=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see here is a function that characterizes the relatve evidence for different values of $\\theta$, conditional on the assumed binomial model (with a uniform prior) and the observed data. Since the data are small, there is still reasonable support across the (0,1) range.\n",
    "\n",
    "Now let's consider a somewhat larger dataset: **9 females from 20 births**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binomial_posterior(n=20, y=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider a whopping **750 births, with 365 girls**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binomial_posterior(n=750, y=365)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as the sample size gets large, the posterior narrows, representing the reduction in residual uncertainty. Notice also that the posterior distributions start to look more and more Gaussian!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [(5, 3), (20, 9), (750, 365)]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=[f\"n={n}, y={y}\" for n, y in pairs]\n",
    ")\n",
    "\n",
    "for i, (n, y) in enumerate(pairs, start=1):\n",
    "    post = theta_range**y * (1 - theta_range)**(n - y)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=theta_range, y=post, mode='lines'),\n",
    "        row=1, col=i\n",
    "    )\n",
    "    fig.update_xaxes(title_text=\"θ\", row=1, col=i)\n",
    "fig.update_yaxes(title_text=\"Posterior Density\", row=1, col=1)\n",
    "fig.update_layout(\n",
    "    title_text=\"Binomial Posterior Distributions\",\n",
    "    showlegend=False,\n",
    "    width=900, height=300\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this simple example, I chose a uniform prior simply because it was easy to work with. But in practice, we often want to choose a prior that is more informative. Can we specify a valid prior that results in a posterior distribution that is simple to work with?\n",
    "\n",
    "Taking our binomial likelihood again as an example:\n",
    "\n",
    "$$P(\\theta | n, y) \\propto \\theta^y (1-\\theta)^{n-y}$$\n",
    "\n",
    "we can see that it is of the general form $\\theta^a (1-\\theta)^b$. Thus, we are looking for a parametric distribution that describes the distribution of or uncertainty in $\\theta$ that is of this general form. The **beta distribution** satisfies these criteria:\n",
    "\n",
    "$$P(\\theta | \\alpha, \\beta) \\propto \\theta^{\\alpha-1} (1-\\theta)^{\\beta-1}$$\n",
    "\n",
    "![](images/Beta_distribution_pdf.png)\n",
    "\n",
    "The parameters $\\alpha, \\beta$ are called **hyperparameters**, and here they suggest prior information corresponding to $\\alpha-1$ \"successes\" and $\\beta-1$ failures. \n",
    "\n",
    "Let's go ahead and calculate the posterior distribution:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "P(\\theta | n, y) &\\propto& \\theta^y (1-\\theta)^{n-y} \\theta^{\\alpha-1} (1-\\theta)^{\\beta-1} \\\\\n",
    "    &=& \\theta^{y+\\alpha-1} (1-\\theta)^{n-y+\\beta-1} \\\\\n",
    "    &=& \\text{Beta}(\\alpha + y, \\beta + n -y) \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "So, in this instance, the posterior distribution follows the same functional form as the prior. This phenomenon is referred to as **conjugacy**, whereby the beta distribution is in the conjugate family for the binomial sampling distribution.\n",
    "\n",
    "> What is the posterior distribution when a Beta(1,1) prior is used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta\n",
    "\n",
    "theta = np.linspace(0, 1, 100)\n",
    "\n",
    "pdf = beta.pdf(theta, 1, 1)\n",
    "\n",
    "go.Figure().add_trace(go.Scatter(\n",
    "    x=theta, y=pdf,\n",
    "    mode='lines',\n",
    "    line=dict(color='red', width=2),\n",
    "    name='Beta(1,1)'\n",
    ")).add_trace(go.Scatter(\n",
    "    x=theta, y=pdf,\n",
    "    fill='tozeroy',\n",
    "    fillcolor='rgba(255,0,0,0.2)',\n",
    "    mode='none',\n",
    "    showlegend=False\n",
    ")).update_layout(\n",
    "    xaxis_title='θ',\n",
    "    yaxis_title='Density',\n",
    "    yaxis=dict(range=[0, 1.5]),\n",
    "    width=600,\n",
    "    height=400\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, we defined conjugacy by saying that a class $\\mathcal{P}$ is a conjugate prior for the class $\\mathcal{F}$ of likelihoods if:\n",
    "\n",
    "$$P(\\theta | y) \\propto f(y|\\theta) p(\\theta) \\in \\mathcal{P} \\text{ for all } f \\in \\mathcal{F} \\text{ and } p \\in \\mathcal{P}$$\n",
    "\n",
    "This definition is quite vague for practical application, so we are more interested in **natural** conjugates, whereby the conjugacy is specific to a particular distribution, and not just a class of distributions.\n",
    "\n",
    "In the case of the binomial model with a beta prior, we can now analytically calculate the posterior mean and variance for the model:\n",
    "\n",
    "$$E[\\theta|n,y] = \\frac{\\alpha + y}{\\alpha + \\beta + n}$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\text{Var}[\\theta|n,y] &=& \\frac{(\\alpha + y)(\\beta + n - y)}{(\\alpha + \\beta + n)^2(\\alpha + \\beta + n +1)} \\\\\n",
    "&=& \\frac{E[\\theta|n,y] (1-E[\\theta|n,y])}{\\alpha + \\beta + n +1}\n",
    "\\end{aligned}$$\n",
    "\n",
    "Notice that the posterior expectation will always fall between the sample and prior means.\n",
    "\n",
    "Notice also what happens when $y$ and $n-y$ get large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior and Likelihood Selection\n",
    "\n",
    "In the previous example, we were able to select a prior distribution that was conjugate to the likelihood. This is a very useful property, as it allows us to calculate the posterior distribution analytically. However, this is not always possible or desirable.\n",
    "\n",
    "Broadly speaking, what criteria should we use to select a prior distribution or a likelihood function?\n",
    "\n",
    "* Appropriate domain (positive valued, 0-1 bounded)\n",
    "* Flexibility to model the data (skew, heavy tail, zero inflation)\n",
    "* Parameterizations that make it easy to specify priors with appropriate location, shape, tail behavior\n",
    "\n",
    "### Common distributions for likelihoods\n",
    "\n",
    "#### For continuous measurements:\n",
    "\n",
    "**Normal**: The most common distribution for continuous data, symmetric bell-shaped curve\n",
    "  - μ (mu): Mean or center of the distribution\n",
    "  - σ (sigma): Standard deviation, controls the spread/width of the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "for mu, sigma in [(0, 1), (0, 2), (2, 1)]:\n",
    "    normal_dist = pm.Normal.dist(mu=mu, sigma=sigma)\n",
    "    pdf_values = np.exp(pm.logp(normal_dist, x).eval())\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x,\n",
    "            y=pdf_values,\n",
    "            mode='lines',\n",
    "            name=f\"Normal(μ={mu}, σ={sigma})\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Normal Distribution\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"Probability density\",\n",
    "    width=600\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lognormal**: For right-skewed data that can't be negative, created when the logarithm of a variable follows a normal distribution\n",
    "  - μ (mu): Mean of the variable's natural logarithm\n",
    "  - σ (sigma): Standard deviation of the variable's natural logarithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "for mu_val, sigma_val in [(0, 0.5), (0, 1), (1, 0.5)]:\n",
    "    lognormal_dist = pm.Lognormal.dist(mu=mu_val, sigma=sigma_val)\n",
    "    pdf_values = np.exp(pm.logp(lognormal_dist, x).eval())\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x,\n",
    "        y=pdf_values,\n",
    "        mode='lines',\n",
    "        name=f\"Lognormal(μ={mu_val}, σ={sigma_val})\"\n",
    "    ))\n",
    "fig.update_layout(\n",
    "    title=\"Lognormal Distribution\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"Probability density\",\n",
    "    width=600,\n",
    "    xaxis_range=[-0.5, 5]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StudentT**: Similar to normal but with heavier tails, useful for data with outliers\n",
    "  - ν (nu): Degrees of freedom, controls how heavy the tails are (smaller values = heavier tails)\n",
    "  - μ (mu): Location parameter (similar to mean)\n",
    "  - σ (sigma): Scale parameter (similar to standard deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "# Add standard normal distribution for comparison\n",
    "normal_dist = pm.Normal.dist(mu=0, sigma=1)\n",
    "normal_pdf = np.exp(pm.logp(normal_dist, x).eval())\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x, y=normal_pdf, mode='lines',\n",
    "        name=\"Normal(μ=0, σ=1)\",\n",
    "        line=dict(color='grey', dash='dash')\n",
    "    )\n",
    ")\n",
    "\n",
    "for nu in [1, 3, 10]:\n",
    "    student_t = pm.StudentT.dist(nu=nu, mu=0, sigma=1)\n",
    "    pdf = np.exp(pm.logp(student_t, x).eval())\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x, y=pdf, mode='lines',\n",
    "            name=f\"StudentT(ν={nu}, μ=0, σ=1)\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Student's T Distribution\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"Probability density\",\n",
    "    width=600\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For durations:\n",
    "\n",
    "**Exponential**: Models time between events in a Poisson process, memoryless property\n",
    "  - λ (lambda): Rate parameter, average number of events per unit time (1/λ is the mean waiting time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "x = np.linspace(0, 5, 1000)\n",
    "\n",
    "for lam in [0.5, 1, 2]:\n",
    "    exponential_dist = pm.Exponential.dist(lam=lam)\n",
    "    pdf_values = np.exp(pm.logp(exponential_dist, x).eval())\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x,\n",
    "            y=pdf_values,\n",
    "            mode='lines',\n",
    "            name=f\"Exponential(λ={lam})\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Exponential Distribution\",\n",
    "    xaxis_title=\"Time\",\n",
    "    yaxis_title=\"Probability density\",\n",
    "    width=600\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For counts:\n",
    "\n",
    "We have already seen one of the most common distributions for modeling bounded counts:\n",
    "\n",
    "- Binomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Poisson**: Models count of events occurring in fixed time interval, mean equals variance\n",
    "  - λ (lambda): Rate parameter, average number of events in the interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 15)\n",
    "fig = go.Figure()\n",
    "\n",
    "for mu in [1, 4, 8]:\n",
    "    pois = pm.Poisson.dist(mu=mu)\n",
    "    pmf = np.exp(pm.logp(pois, x).eval())\n",
    "    offset = 0.2 * (mu/4 - 1)\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=x + offset,\n",
    "        y=pmf,\n",
    "        width=0.2,\n",
    "        opacity=0.6,\n",
    "        name=f\"Poisson(λ={mu})\"\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Poisson Distribution\",\n",
    "    xaxis_title=\"Count\",\n",
    "    yaxis_title=\"Probability\",\n",
    "    barmode=\"overlay\",\n",
    "    width=600\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Negative binomial**: Useful for overdispersed count data (variance > mean)\n",
    "  - μ (mu): Mean parameter\n",
    "  - α (alpha): Dispersion parameter - smaller values indicate greater overdispersion\n",
    "  - Variance = μ + μ²/α"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 21)\n",
    "fig = go.Figure()\n",
    "\n",
    "for n, p in [(5, 0.5), (10, 0.5), (5, 0.7)]:\n",
    "    nb_dist = pm.NegativeBinomial.dist(mu=n*(1-p)/p, alpha=n)\n",
    "    pmf = np.exp(pm.logp(nb_dist, x).eval())\n",
    "    offset = 0.2 * (p * 10 - 3)\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=x + offset,\n",
    "        y=pmf,\n",
    "        width=0.2,\n",
    "        name=f\"NegBinom(n={n}, p={p})\",\n",
    "        opacity=0.6\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Negative Binomial Distribution\",\n",
    "    xaxis_title=\"Number of failures\",\n",
    "    yaxis_title=\"Probability\",\n",
    "    barmode=\"overlay\",\n",
    "    width=600\n",
    ")\n",
    "fig.update_xaxes(tickmode=\"array\", tickvals=x[::2])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common distributions for priors\n",
    "\n",
    "#### For location parameters like $\\mu$:\n",
    "\n",
    "Location parameters represent real-valued quantities like means or medians. So we can use a few distributions that we have already seen:\n",
    "\n",
    "* Normal\n",
    "* Lognormal\n",
    "* StudentT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For scale parameters like $\\sigma$:\n",
    "\n",
    "Scale parameters represent positive, continuous quantities like standard deviations.\n",
    "\n",
    "We have already seen one distribution that meets this criterion:\n",
    "- Exponential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HalfNormal**: A Normal distribution truncated at zero. It's often used for standard deviations or other scale parameters that must be positive.\n",
    "  - $\\sigma$ (`sigma`): Scale parameter, representing the standard deviation of the underlying Normal distribution before truncation.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 5, 500)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for sigma_val in [0.5, 1, 2]:\n",
    "    half_normal = pm.HalfNormal.dist(sigma=sigma_val)\n",
    "    pdf_values = np.exp(pm.logp(half_normal, x).eval())\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x,\n",
    "        y=pdf_values,\n",
    "        mode='lines',\n",
    "        name=f\"HalfNormal(σ={sigma_val})\"\n",
    "    ))\n",
    "fig.update_layout(\n",
    "    title=\"HalfNormal Distribution\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"Probability density\",\n",
    "    width=600\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "#### For proportions or probabilities:\n",
    "\n",
    "These distributions are suitable for parameters that are bounded between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beta**: A continuous probability distribution defined on the interval [0, 1]. It's widely used to model probabilities or proportions.\n",
    "  - $\\alpha$ (`alpha`): Shape parameter (can be seen as prior successes + 1).\n",
    "  - $\\beta$ (`beta`): Shape parameter (can be seen as prior failures + 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "x = np.linspace(0, 1, 500)\n",
    "for alpha_val, beta_val in [(0.5, 0.5), (5, 1), (1, 3), (2, 2), (5, 5)]:\n",
    "    beta_dist = pm.Beta.dist(alpha=alpha_val, beta=beta_val)\n",
    "    pdf_values = np.exp(pm.logp(beta_dist, x).eval())\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x, y=pdf_values,\n",
    "        mode='lines',\n",
    "        name=f\"Beta(α={alpha_val}, β={beta_val})\"\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Beta Distribution\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"Probability density\",\n",
    "    width=600\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": [
    "#### For rates (e.g., $\\lambda > 0$)\n",
    "Rates are positive, continuous parameters. So we can use a few distributions that we have already seen:\n",
    "\n",
    " - Lognormal\n",
    " - Exponential\n",
    "\n",
    "**Gamma**: A two-parameter continuous probability distribution for positive-valued random variables. This is a generalization of the exponential distribution, and is often used to model waiting times or rates.\n",
    "  - $\\alpha$ (`alpha`): Shape parameter.\n",
    "  - $\\beta$ (`beta`): Rate parameter (PyMC defines $\\beta$ as the rate; $1/\\beta$ is the scale or mean/alpha)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "x = np.linspace(0.01, 10, 500)\n",
    "\n",
    "for alpha_val, beta_val in [(1, 1), (2, 1), (3, 2), (5, 1), (1, 0.5)]:\n",
    "    gamma_dist = pm.Gamma.dist(alpha=alpha_val, beta=beta_val)\n",
    "    pdf_values = np.exp(pm.logp(gamma_dist, x).eval())\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x, \n",
    "        y=pdf_values, \n",
    "        mode='lines', \n",
    "        name=f\"Gamma(α={alpha_val}, β={beta_val})\"\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Gamma Distribution\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"Probability density\",\n",
    "    width=600\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "source": [
    "**InverseGamma**: A two-parameter continuous probability distribution for positive-valued random variables. It's the distribution of the reciprocal of a Gamma-distributed variable. Often used as a prior for variance parameters.\n",
    "  - $\\alpha$ (`alpha`): Shape parameter.\n",
    "  - $\\beta$ (`beta`): Scale parameter (PyMC defines $\\beta$ as the scale for InverseGamma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "x = np.linspace(0.01, 5, 500)\n",
    "\n",
    "for alpha_val, beta_val in [(1, 1), (2, 1), (3, 1), (3, 0.5)]:\n",
    "    inv_gamma_dist = pm.InverseGamma.dist(alpha=alpha_val, beta=beta_val)\n",
    "    pdf_values = np.exp(pm.logp(inv_gamma_dist, x).eval())\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x, \n",
    "            y=pdf_values, \n",
    "            mode='lines', \n",
    "            name=f\"InverseGamma(α={alpha_val}, β={beta_val})\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"InverseGamma Distribution\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"Probability density\",\n",
    "    width=600\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a feel for how model variables can be represented, let's build our first model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Estimation for One Group\n",
    "\n",
    "For this we will use Gelman et al.'s (2007) radon dataset. In this dataset the amount of the radioactive gas radon has been measured among different households in all counties of several states. Radon gas is known to be the highest cause of lung cancer in non-smokers. It is believed to be more strongly present in households containing a basement and to differ in amount present among types of soil.\n",
    "\n",
    ">  the US EPA has set an action level of 4 pCi/L. At or above this level of radon, the EPA recommends you take corrective measures to reduce your exposure to radon gas.\n",
    "\n",
    "![radon](images/how_radon_enters.jpg?raw=true)\n",
    "\n",
    "Let's import the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radon = pl.read_csv('../data/radon.csv')\n",
    "radon.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on the (log) radon levels measured in a single county (Hennepin). \n",
    "\n",
    "Suppose we are interested in:\n",
    "\n",
    "- whether the mean log-radon value is greater than 4 pCi/L in Hennepin county\n",
    "- the probability that any randomly-chosen household in Hennepin county has a reading of greater than 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hennepin_radon = radon.filter(pl.col('county') == 'HENNEPIN').select('log_radon')\n",
    "\n",
    "px.histogram(hennepin_radon, nbins=20,\n",
    "                  x='log_radon',\n",
    "                  title='Distribution of Log Radon Levels in Hennepin County'\n",
    ").update_layout(\n",
    "    showlegend=False,\n",
    "    xaxis_title='Log Radon Level',\n",
    "    yaxis_title='Count',\n",
    "    width=600\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model\n",
    "\n",
    "Once again, the first step in Bayesian inference is specifying a **full probability model** for the problem.\n",
    "\n",
    "This consists of:\n",
    "\n",
    "- a likelihood function(s) for the observations\n",
    "- priors for all unknown quantities\n",
    "\n",
    "The measurements look approximately normal, so let's start by assuming a normal distribution as the sampling distribution (likelihood) for the data. \n",
    "\n",
    "$$y_i \\sim N(\\mu, \\sigma^2)$$\n",
    "\n",
    "(don't worry, we can evaluate this assumption)\n",
    "\n",
    "This implies that we have 2 unknowns in the model; the mean and standard deviation of the distribution. \n",
    "\n",
    "#### Prior choice\n",
    "\n",
    "While there may likely be prior information about the distribution of radon values, we will assume no prior knowledge, and specify a **diffuse** prior for each parameter.\n",
    "\n",
    "Since the mean can take any real value (since it is on the log scale), we will use another normal distribution here, and specify a large variance to allow the possibility of very large or very small values:\n",
    "\n",
    "$$\\mu \\sim N(0, 10^2)$$\n",
    "\n",
    "For the standard deviation, we know that the true value must be positive (no negative variances!). I will choose a uniform prior bounded from below at zero and from above at a value that is sure to be higher than any plausible value the true standard deviation (on the log scale) could take.\n",
    "\n",
    "$$\\sigma \\sim U(0, 10)$$\n",
    "\n",
    "We can encode these in a Python model, using the PyMC package, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as radon_model:\n",
    "    \n",
    "    mu = pm.Normal('mu', mu=0, sigma=10)\n",
    "    sigma = pm.Uniform('sigma', 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Software\n",
    "> Today there is an array of software choices for Bayesians, including both open source software (*e.g.*, Stan, PyMC, Pyro, TensorFlow Probability) and commercial (*e.g.*, SAS, Stata). These examples can be replicated in any of these environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that remains is to add the likelihood, which takes $\\mu$ and $\\sigma$ as parameters, and the log-radon values as the set of observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with radon_model:\n",
    "    \n",
    "    dist = pm.Normal('dist', mu=mu, sigma=sigma, observed=hennepin_radon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go ahead and estimate the model paramters from the data, it's a good idea to perform a **prior predictive check**. This involves sampling from the model before data are incorporated, and gives you an idea of the range of observations that would be considered reasonable within the scope of the modeling assumptions (including choice of priors). If the simnulations generate too many extreme observations relative to our expectations based on domain knowledge, then it can be an indication of problems with model formulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with radon_model:\n",
    "    \n",
    "    prior_sample = pm.sample_prior_predictive(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_values = prior_sample.prior_predictive['dist'].values.flatten()\n",
    "\n",
    "px.histogram(\n",
    "    x=dist_values,\n",
    "    nbins=20,\n",
    "    title=\"Prior Predictive Distribution\",\n",
    "    labels={\"x\": \"Value\"},\n",
    "    opacity=0.7\n",
    ").update_layout(\n",
    "    xaxis_title=\"Value\",\n",
    "    yaxis_title=\"Count\",\n",
    "    width=600,\n",
    "    height=400\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly compare this to the data values, especially checking the minimum and maximum values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radon[\"log_radon\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will fit the model using **Markov chain Monte Carlo (MCMC)**, which will be covered in detail in an upcoming section. This will draw values from a sampler that (we home) has coverged to the true posterior distribution (which cannot be calculated exactly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with radon_model:\n",
    "    \n",
    "    samples = pm.sample(1000, tune=1000, cores=2, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having fit the model, we can look at the posterior estimate of the mean relative to the critical value (on the log scale).\n",
    "\n",
    "The plot shows the posterior distribution of $\\mu$, along with an estimate of the 94% posterior **credible interval**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(samples, var_names=['mu'], ref_val=np.log(4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output\n",
    "\n",
    "    82% < 1.38629 < 18%\n",
    "    \n",
    "informs us that the probability of $\\mu$ being less than log(4) is 82% and the corresponding probability of being greater than log(4) is about 18%.\n",
    "\n",
    "> The posterior probability that the mean level of household radon in Henneprin County is greater than 4 pCi/L is 0.178."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "What is the probability that a given household has a log-radon measurement larger than four? To answer this, we make use of the **posterior predictive distribution**.\n",
    "\n",
    "$$p(z |y) = \\int_{\\theta} p(z |\\theta) p(\\theta | y) d\\theta$$\n",
    "\n",
    "where here $z$ is the predicted value and y is the data used to fit the model.\n",
    "\n",
    "While PyMC has functions for calculating the posterior predictive distribution, we can readily estimate this from the posterior samples of the parameters in the model. First, extract the values of the mean and standard deviation of the sampling distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_data = az.extract(samples.posterior)\n",
    "\n",
    "mus = posterior_data['mu'].values\n",
    "sigmas = posterior_data['sigma'].values\n",
    "\n",
    "print(\"The shape of the mus array is:\", mus.shape)\n",
    "print(\"The shape of the sigmas array is:\", sigmas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "radon_samples = stats.norm(loc=mus, scale=sigmas).rvs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(radon_samples > np.log(4)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The posterior probability that a randomly-selected household in Henneprin County contains radon levels in excess of 4 pCi/L is about 0.46."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model checking\n",
    "\n",
    "But, ***how do we know this model is any good?***\n",
    "\n",
    "Its important to check the fit of the model, to see if its assumptions are reasonable. One way to do this is to perform **posterior predictive checks**. This involves generating simulated data using the model that you built, and comparing that data to the observed data.\n",
    "\n",
    "One can choose a particular statistic to compare, such as tail probabilities or quartiles, but here it is useful to compare them graphically.\n",
    "\n",
    "We already have these simulations from the previous exercise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated data\n",
    "x_sim = np.linspace(min(radon_samples) - 1, max(radon_samples) + 1, 1000)\n",
    "kde_sim = stats.gaussian_kde(radon_samples)\n",
    "y_sim = kde_sim(x_sim)\n",
    "\n",
    "# Observed data\n",
    "x_obs = np.linspace(min(hennepin_radon['log_radon'].to_numpy()) - 1, max(hennepin_radon['log_radon'].to_numpy()) + 1, 1000)\n",
    "kde_obs = stats.gaussian_kde(hennepin_radon['log_radon'].to_numpy())\n",
    "y_obs = kde_obs(x_obs)\n",
    "\n",
    "go.Figure().add_trace(go.Scatter(\n",
    "    x=x_sim,\n",
    "    y=y_sim,\n",
    "    mode='lines',\n",
    "    name='Simulated',\n",
    "    line=dict(color='red', width=2)\n",
    ")).add_trace(go.Scatter(\n",
    "    x=x_obs,\n",
    "    y=y_obs,\n",
    "    mode='lines',\n",
    "    name='Observed',\n",
    "    line=dict(color='blue', width=2)\n",
    ")).add_shape(\n",
    "    type=\"line\",\n",
    "    x0=np.log(4),\n",
    "    y0=0,\n",
    "    x1=np.log(4),\n",
    "    y1=max(max(y_sim), max(y_obs)),\n",
    "    line=dict(color=\"black\", dash=\"dash\"),\n",
    ").add_annotation(\n",
    "    x=np.log(4),\n",
    "    y=max(max(y_sim), max(y_obs))/2,\n",
    "    text=\"log(4)\",\n",
    "    showarrow=True,\n",
    "    arrowhead=1,\n",
    "    ax=30,\n",
    "    ay=0\n",
    ").update_layout(\n",
    "    title='KDE: Observed vs Simulated Log Radon Levels',\n",
    "    xaxis_title='Log Radon Level',\n",
    "    yaxis_title='Density',\n",
    "    width=600,\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"right\",\n",
    "        x=0.99\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Estimation for Two Groups\n",
    "\n",
    "Let's move on to a slightly different problem: rather than estimating a single group's distribution, what if we have data from two unknown distributions and we want to estimate how different they are.\n",
    "\n",
    "We will use a fictitious example from Kruschke (2012) concerning the evaluation of a clinical trial for drug evaluation. The trial aims to evaluate the efficacy of a \"smart drug\" that is supposed to increase intelligence by comparing IQ scores of individuals in a treatment arm (those receiving the drug) to those in a control arm (those recieving a placebo). There are 47 individuals and 42 individuals in the treatment and control arms, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug = pl.DataFrame(dict(iq=(101,100,102,104,102,97,105,105,98,101,100,123,105,103,100,95,102,106,\n",
    "        109,102,82,102,100,102,102,101,102,102,103,103,97,97,103,101,97,104,\n",
    "        96,103,124,101,101,100,101,101,104,100,101),\n",
    "                         group='drug'))\n",
    "placebo = pl.DataFrame(dict(iq=(99,101,100,101,102,100,97,101,104,101,102,102,100,105,88,101,100,\n",
    "           104,100,100,100,101,102,103,97,101,101,100,101,99,101,100,100,\n",
    "           101,100,99,101,100,102,99,100,99),\n",
    "                            group='placebo'))\n",
    "n1 = len(drug)\n",
    "n0 = len(placebo)\n",
    "\n",
    "trial_data = pl.concat([drug, placebo])\n",
    "\n",
    "# Create histogram using plotly express directly with polars DataFrame\n",
    "fig = px.histogram(\n",
    "    trial_data, \n",
    "    x=\"iq\", \n",
    "    color=\"group\",\n",
    "    barmode=\"overlay\",\n",
    "    histnorm=\"percent\",\n",
    "    labels={\"iq\": \"IQ Score\", \"group\": \"Group\"},\n",
    "    title=\"Distribution of IQ Scores by Group\",\n",
    "    color_discrete_map={\"drug\": \"#636EFA\", \"placebo\": \"#EF553B\"}\n",
    ")\n",
    "\n",
    "# Update layout for better appearance\n",
    "fig.update_layout(\n",
    "    legend_title_text=\"\",\n",
    "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
    "    xaxis_title=\"IQ Score\",\n",
    "    yaxis_title=\"Percentage\"\n",
    ")\n",
    "\n",
    "iq = trial_data.select(\"iq\").to_numpy().squeeze()\n",
    "group = trial_data[\"group\"].cast(pl.Categorical).to_physical().to_numpy().squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there appear to be extreme (\"outlier\") values in the data, we will choose a Student-t distribution to describe the distributions of the scores in each group (see the distribution gallery above). This sampling distribution adds **robustness** to the analysis, as a T distribution is less sensitive to outlier observations, relative to a normal distribution. \n",
    "\n",
    "The three-parameter Student-t distribution allows for the specification of a mean $\\mu$, a precision (inverse-variance) $\\lambda$ and a degrees-of-freedom parameter $\\nu$:\n",
    "\n",
    "$$f(x|\\mu,\\lambda,\\nu) = \\frac{\\Gamma(\\frac{\\nu + 1}{2})}{\\Gamma(\\frac{\\nu}{2})} \\left(\\frac{\\lambda}{\\pi\\nu}\\right)^{\\frac{1}{2}} \\left[1+\\frac{\\lambda(x-\\mu)^2}{\\nu}\\right]^{-\\frac{\\nu+1}{2}}$$\n",
    "           \n",
    "the degrees-of-freedom parameter essentially specifies the \"normality\" of the data, since larger values of $\\nu$ make the distribution converge to a normal distribution, while small values (close to zero) result in heavier tails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-4, 4, 1000)\n",
    "\n",
    "normal_dist = stats.norm.pdf(x, 0, 1)\n",
    "t_dist = stats.t.pdf(x, df=3)\n",
    "\n",
    "go.Figure().add_trace(go.Scatter(\n",
    "    x=x,\n",
    "    y=normal_dist,\n",
    "    mode='lines',\n",
    "    name='Normal(0,1)',\n",
    "    line=dict(color='blue', width=2)\n",
    ")).add_trace(go.Scatter(\n",
    "    x=x,\n",
    "    y=t_dist,\n",
    "    mode='lines',\n",
    "    name='Student-t(df=3)',\n",
    "    line=dict(color='red', width=2)\n",
    ")).update_layout(\n",
    "    title='Normal vs Student-t Distribution',\n",
    "    xaxis_title='x',\n",
    "    yaxis_title='Density',\n",
    "    width=600,\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\", \n",
    "        y=1.02, \n",
    "        xanchor=\"right\", \n",
    "        x=1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Thus, the likelihood functions of our model are specified as follows:\n",
    "\n",
    "$$\\begin{align}\n",
    "y^{(drug)}_i &\\sim T(\\nu, \\mu_1, \\sigma_1) \\\\\n",
    "y^{(placebo)}_i &\\sim T(\\nu, \\mu_2, \\sigma_2)\n",
    "\\end{align}$$\n",
    "\n",
    "As a simplifying assumption, we will assume that the degree of normality $\\nu$ is the same for both groups. \n",
    "\n",
    "### Prior choice\n",
    "\n",
    "Since the means are real-valued, we will apply normal priors. Since we know something about the population distribution of IQ values, we will center the priors at 100, and use a standard deviation that is more than wide enough to account for plausible deviations from this population mean:\n",
    "\n",
    "$$\\mu_k \\sim N(100, 10^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as drug_model:\n",
    "    \n",
    "    mu = pm.Normal('mu', 100, sigma=10, shape=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Here is a random sample from the mu prior:\", pm.draw(mu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we will use a uniform prior for the standard deviations, with an upper bound of 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with drug_model:\n",
    "    \n",
    "    sigma = pm.Uniform('sigma', lower=0, upper=20, shape=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the overdispersion parameter $\\nu$, we will use an **exponential** distribution with a mean of 30; this allocates high prior probability over the regions of the parameter that describe the range from normal to heavy-tailed data under the Student-T distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with drug_model:\n",
    "    nu = pm.Exponential('nu', 1/30) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pm.draw(nu, 10_000)\n",
    "\n",
    "fig = px.histogram(\n",
    "    x=samples, \n",
    "    nbins=50, \n",
    "    title=\"Exponential Distribution (1/30)\",\n",
    "    labels={\"x\": \"Value\", \"count\": \"Frequency\"},\n",
    "    opacity=0.7\n",
    ").update_layout(\n",
    "    xaxis_title=\"Value\",\n",
    "    yaxis_title=\"Frequency\",\n",
    "    width=600,\n",
    "    showlegend=False\n",
    ").update_xaxes(range=[0, 100])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with drug_model:\n",
    "\n",
    "    like = pm.StudentT('placebo_like', nu=nu, mu=mu[group], sigma=sigma[group], observed=iq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is fully specified, we can turn our attention to tracking the posterior quantities of interest. Namely, we can calculate the difference in means between the drug and placebo groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with drug_model:\n",
    "    \n",
    "    diff_of_means = pm.Deterministic('difference of means', mu[0] - mu[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with drug_model:\n",
    "    \n",
    "    drug_trace = pm.sample(random_seed=RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(drug_trace, \n",
    "          var_names=['difference of means'],\n",
    "          ref_val=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The posterior probability that the mean IQ of subjects in the treatment group is greater than that of the control group is approximately 0.99."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# References\n",
    "\n",
    "Gelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. [Bayesian Data Analysis, Third Edition](http://www.stat.columbia.edu/~gelman/book/). CRC Press.\n",
    "\n",
    "Downey, Allen. 2021. [Think Bayes: Bayesian Statistics in Python, Second Edition](https://allendowney.github.io/ThinkBayes2/). O'Reilly Media.\n",
    "\n",
    "Pilon, Cam-Davidson. [Probabilistic Programming and Bayesian Methods for Hackers](http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
