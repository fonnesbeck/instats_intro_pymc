{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2: PyMC and PyTensor\n",
    "\n",
    "In this session, we'll explore the fundamental components of PyMC: PyTensor and PyMC's variable classes. We'll learn how PyTensor defines and optimizes computational graphs, and how PyMC uses these capabilities to build probabilistic models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTensor Basics\n",
    "\n",
    "PyTensor is the computational backend for PyMC. It defines symbolic variables and operations on them, which are compiled into efficient functions that can run on CPUs or GPUs. Let's begin by exploring the basic elements of PyTensor.\n",
    "\n",
    "In PyTensor, you define a computational graph explicitly. You start with input variables that are essentially placeholders and from these, build intermediate variables by applying operators. These intermediate variables can then be treated as final outputs or as inputs for further computation.\n",
    "\n",
    "While PyTensor is designed to feel similar to NumPy to ease the learning curve, it's important to remember they are distinct. PyTensor operations build a graph of computations (which are executed lazily) rather than immediately returning values like NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors and Basic Operations\n",
    "\n",
    "To begin, let's define some PyTensor tensors and show how to perform some basic operations.\n",
    "\n",
    "A tensor can be a scalar or a vector with any number of dimensions.\n",
    "\n",
    "Concretely:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytensor\n",
    "import pytensor.tensor as pt\n",
    "import numpy as np\n",
    "\n",
    "x = pt.tensor(shape=(), dtype=\"float64\")\n",
    "y = pt.tensor(shape=(2,), dtype=\"float64\")\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "x type: {x.type}\n",
    "x shape = {x.type.shape}\n",
    "---\n",
    "y type: {y.type}\n",
    "y shape = {y.type.shape}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the `x` and `y` tensors, we can create a new one by adding them together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = x + y\n",
    "z.name = \"x + y\"\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the computation a bit more complex let's take the logarithm of the resulting tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = pt.log(z)\n",
    "w, type(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not give `w` a name, so it prints something more descriptive: its the index-0 output of calling the `log` function. Its type is `TensorVariable`, which is the base class for all PyTensor variables.\n",
    "\n",
    "So PyTensor works something like NumPy, but it builds a graph of operations rather than executing, more like a symbolic computation library.\n",
    "\n",
    "We can use the `pytensor.dprint` function to print the computational graph of any given tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytensor.dprint(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output shows the structure of the computation that PyTensor has built for the variable `w`. Think of it as a recipe (in reverse):\n",
    "\n",
    "- **`Log [id A] 'log(x + y)'`**: This is the final result, `w`. It's calculated by taking the logarithm (`Log`) of an intermediate value named `'log(x + y)'`. PyTensor assigns it an internal identifier `A`.\n",
    "- **`Add [id B] 'x + y'`**: This is the input to the `Log` operation. It's an intermediate value named `'x + y'` (which we called `z` in the code), calculated by an addition (`Add`). Its internal ID is `B`.\n",
    "- **`ExpandDims{axis=0} [id C]`**: This is the first input to the `Add` operation. `ExpandDims` is an operation that changes the shape of a tensor. Here, it's likely making the scalar `x` compatible for addition with the vector `y`. Its ID is `C`.\n",
    "  - **`<Scalar(float64, shape=())> [id D]`**: This is the input to `ExpandDims`. It's our original scalar tensor `x` (ID `D`), which holds a single 64-bit floating-point number.\n",
    "- **`<Vector(float64, shape=(3,))> [id E]`**: This is the second input to the `Add` operation. It's our original vector tensor `y` (ID `E`), which holds three 64-bit floating-point numbers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "Note that this graph does not do any computation (yet!). It is simply defining the sequence of steps to be done. We can use `pytensor.function` to define a callable object so that we can push values through the graph.\n",
    "\n",
    "PyTensor functions are compiled from symbolic expressions into efficient callable functions. The `pytensor.function` constructor takes several key arguments that define how the function will behave:\n",
    "\n",
    "- The `inputs` argument specifies which PyTensor variables will be provided when calling the function. These become the function's parameters.\n",
    "\n",
    "- The `outputs` argument defines which symbolic expressions should be evaluated and returned when the function is called.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pytensor.function(inputs=[x, y], outputs=w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the graph is compiled, we can push some concrete values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(0, [1, np.e])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TIP:\n",
    "> Sometimes we just want to debug, we can use `pytensor.graph.basic.Variable.eval` for that:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.eval({x: 0, y: [1, np.e]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set intermediate values as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.eval({z: [1, np.e]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Optimization\n",
    "\n",
    "One of the most important features of `pytensor` is that it can automatically **optimize** the mathematical operations inside a graph. Let's consider a simple example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pt.tensor(shape=(), name=\"a\")\n",
    "b = pt.tensor(shape=(), name=\"b\")\n",
    "\n",
    "c = a / b\n",
    "c.name = \"a / b\"\n",
    "\n",
    "pytensor.dprint(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's multiply `b` times `c`. This should result in simply `a`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = b * c\n",
    "d.name = \"b * c\"\n",
    "\n",
    "pytensor.dprint(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows the full computation, but once we compile it the operation becomes the identity on `a` as expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = pytensor.function(inputs=[a, b], outputs=d)\n",
    "\n",
    "pytensor.dprint(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is in a PyTensor Graph?\n",
    "\n",
    "The following diagram shows the basic structure of an `pytensor` graph.\n",
    "\n",
    "![pytensor graph](images/apply.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make these concepts more tangible by explicitly indicating them in our earlier example. Let's compute the graph components for the tensor `z`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"\"\"\n",
    "z type: {z.type}\n",
    "z name = {z.name}\n",
    "z owner = {z.owner}\n",
    "z owner inputs = {z.owner.inputs}\n",
    "z owner op = {z.owner.op}\n",
    "z owner output = {z.owner.outputs}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Manipulation\n",
    "\n",
    "Another interesting feature of PyTensor is the ability to manipulate the computational graph, something that is not possible with TensorFlow or PyTorch. Here we'll see how to modify an existing graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get input tensors\n",
    "list(pytensor.graph.graph_inputs(graphs=[w]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple example, let's add an `pytensor.tensor.exp` before the `pytensor.tensor.log` (to get the identity function).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_of_w = w.owner.inputs[0]  # get z tensor\n",
    "new_parent_of_w = pt.exp(parent_of_w)  # modify the parent of w\n",
    "new_parent_of_w.name = \"exp(x + y)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the graph of `w` has actually not changed:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytensor.dprint(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To modify the graph we need to use the `pytensor.clone_replace` function, which _returns a copy of the initial subgraph with the corresponding substitutions._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_w = pytensor.clone_replace(output=[w], replace={parent_of_w: new_parent_of_w})[0]\n",
    "new_w.name = \"log(exp(x + y))\"\n",
    "pytensor.dprint(new_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can test the modified graph by passing some input to the new graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_w.eval({x: 0, y: [1, np.e]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the new graph is just the identity function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE:\n",
    "> Again, note that `pytensor` is clever enough to omit the `exp` and `log` once we compile the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pytensor.function(inputs=[x, y], outputs=new_w)\n",
    "\n",
    "pytensor.dprint(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(0, [1, np.e])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of manipulation is called a **graph rewrite**. Rewrites bridge the gap between allowing users to define graphs in any way they please, while allowing the resulting computation to be carried out in an optimal way, from the standpoint of performance and stability.\n",
    "\n",
    "While some rewrites are performed automatically, we can invoke them manually as well. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pt.tensor('x', shape=(None, ))\n",
    "y = pt.log(1 + x)\n",
    "y.dprint(print_type=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expression for `y` can be numerically unstable when `x` is very close to zero, due to floating point precision.\n",
    "\n",
    "PyTensor's stabilization rewrite replaces such expressions to use more numerically stable equivalents, in this case with `pt.special.log1p(x)` which calculates `log(1 + x)` accurately even for small `x`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_y = pytensor.graph.rewrite_graph(y, include=(\"stabilize\",))\n",
    "stable_y.dprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Logistic Regression\n",
    "\n",
    "Let's try building a simple model in pure PyTensor. We will estimate the parameters of a logistic regression model using a simple version of gradient descent.\n",
    "\n",
    "Gelman et al. (2003) present an example of an acute toxicity test, commonly performed on animals to estimate the toxicity of various compounds.\n",
    "\n",
    "In this dataset `log_dose` includes 4 levels of dosage, on the log scale, each administered to `n=5` rats during the experiment. The response variable is `death`, the number of positive responses to the dosage.\n",
    "\n",
    "The number of deaths can be modeled as a binomial response, with the probability of death being a linear function of dose:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{logit}(p_i) &= a + b x_i \\\\\n",
    "y_i &\\sim \\text{Bin}(n_i, p_i) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The common statistic of interest in such experiments is the **LD50**, the dosage at which the probability of death is 50%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random\n",
    "\n",
    "dose = np.array([-0.86, -0.3, -0.05, 0.73])\n",
    "n = 5\n",
    "deaths = np.array([0, 1, 3, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's declare our symbolic variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pt.tensor(\"X\", shape=(None,))\n",
    "Y = pt.tensor(\"Y\", shape=(None,))\n",
    "a = pt.tensor(\"a\", shape=())\n",
    "b = pt.tensor(\"b\", shape=())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... then construct the expression graph:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability that target = 1\n",
    "p_1 = 1 / (1 + pt.exp(-(a + X * b)))\n",
    "p_1.name = \"prob_target_1\"\n",
    "\n",
    "# The prediction threshold\n",
    "prediction = p_1 > 0.5\n",
    "\n",
    "# Cross-entropy loss function\n",
    "xent = -Y * pt.log(p_1) - (n - Y) * pt.log(1 - p_1)\n",
    "xent.name = \"cross-entropy\"\n",
    "\n",
    "# The cost to minimize\n",
    "cost = xent.mean()\n",
    "\n",
    "# Perform rewrites\n",
    "stable_cost = pytensor.graph.rewrite_graph(cost, include=('canonicalize', 'stabilize'))\n",
    "\n",
    "# Compute the gradient of the cost\n",
    "ga, gb = pt.grad(stable_cost, [a, b])\n",
    "\n",
    "# Learning rate\n",
    "step = pt.tensor(\"step\", shape=())\n",
    "\n",
    "# Update the parameters\n",
    "a_new = a - step * ga\n",
    "b_new = b - step * gb\n",
    "step_new = step * 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in order to use these expressions, we need to compile them into functions.\n",
    "\n",
    "Below we compile two functions: `train()` which performs gradient descent by updating parameters based on input data, and `predict()` which makes predictions using the current parameter values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pytensor.function(\n",
    "    inputs=[X, Y, a, b, step],\n",
    "    outputs=[prediction, xent, a_new, b_new, step_new],\n",
    ")\n",
    "predict = pytensor.function(inputs=[X, a, b], outputs=prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, beta, lr = 0.0, 1.0, 10.0\n",
    "training_steps = 1000\n",
    "for i in range(training_steps):\n",
    "    pred, err, alpha, beta, lr = train(dose, deaths, alpha, beta, lr)\n",
    "\n",
    "print(\"Final model:\", alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def logit(x):\n",
    "    return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "xvals = np.linspace(-1, 1)\n",
    "\n",
    "go.Figure().add_trace(\n",
    "    go.Scatter(\n",
    "        x=xvals, y=logit(beta * xvals + alpha), mode=\"lines\", name=\"Fitted Model\"\n",
    "    )\n",
    ").add_trace(\n",
    "    go.Scatter(\n",
    "        x=dose,\n",
    "        y=[d / n for d in deaths],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(color=\"red\", size=10),\n",
    "        name=\"Observed Data\",\n",
    "    )\n",
    ").update_layout(\n",
    "    title=\"Logistic Regression Model\",\n",
    "    xaxis_title=\"Log Dose\",\n",
    "    yaxis_title=\"Probability of Death\",\n",
    "    width=600,\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The PyMC API\n",
    "\n",
    "Bayesian inference begins with specification of a probability model relating unknown variables to data. PyMC provides the basic building blocks for Bayesian probability models: stochastic random variables, deterministic variables, and factor potentials.\n",
    "\n",
    "A **stochastic random variable** is a factor whose value is not completely determined by its parents, while the value of a **deterministic random variable** is entirely determined by its parents. Most models can be constructed using only these two variable types. The third quantity, the **factor potential**, is _not_ a variable but simply a\n",
    "log-likelihood term or constraint that is added to the joint log-probability to modify it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Distribution Class\n",
    "\n",
    "A stochastic variable is represented in PyMC by a `Distribution` class. This structure adds functionality to Pytensor's `pytensor.tensor.random.op.RandomVariable` class, mainly by registering it with an associated PyMC `Model` -- so `Distribution` objects are only usable inside of a `Model` context.\n",
    "\n",
    "`Distribution` subclasses (i.e. implementations of specific statistical distributions) will accept several arguments when constructed:\n",
    "\n",
    "`name`\n",
    ": Name for the new model variable. This argument is **required**, and is used as a label and index value for the variable.\n",
    "\n",
    "`model`\n",
    ": The PyMC model to which the variable belongs.\n",
    "\n",
    "`shape`\n",
    ": The variable's shape.\n",
    "\n",
    "`total_size`\n",
    ": The overall size of the variable (this variable will not exist for scalars).\n",
    "\n",
    "`dims`\n",
    ": A tuple of dimension names known to the model.\n",
    "\n",
    "`transform`\n",
    ": A transformation to be applied to the distribution when used by the model, especially when the distribution is constrained.\n",
    "\n",
    "`initval`\n",
    ": Numeric or symbolic untransformed initial value of matching shape, or one of the following initial value strategies: \"moment\", \"prior\". Depending on the sampler's settings, a random jitter may be added to numeric, symbolic or moment-based initial values in the transformed space.\n",
    "\n",
    "Sometimes we wish to use a particular statistical distribution, without using it as a variable in a model; for example, to generate random numbers from the distribution. For this purpose, `Distribution` objects have a method `dist` that returns a **stateless** probability distribution of that type; that is, without being wrapped in a PyMC random variable object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import plotly.express as px\n",
    "\n",
    "x = pm.Exponential.dist(1)\n",
    "samples = pm.draw(x, draws=1000)\n",
    "\n",
    "fig = px.histogram(samples, title=\"Exponential Distribution Samples\")\n",
    "fig.update_layout(xaxis_title=\"Value\", yaxis_title=\"Count\", showlegend=False)\n",
    "fig.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Models in PyMC\n",
    "\n",
    "Now that we understand the basic building blocks of PyMC models, let's see how to combine them to build a complete model. We'll use a real-world example of predicting college basketball game outcomes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NCAA Basketball Matchup Model\n",
    "\n",
    "College basketball is an immensely popular sport, and the annual \"March Madness\" NCAA tournament is a staple of the American sports calendar. Each year, millions of fans fill out NCAA tournament brackets, trying to predict the outcomes of each round of the knockout tournament.\n",
    "\n",
    "We will use a dataset of NCAA basketball games from the 2017-18 season to build a model that predicts the outcome of a given game based on the strengths of the competing teams. Conceivably, this model could be used to help fill out a bracket, or to make a prediction for the outcome of individual games.\n",
    "\n",
    "![2018 NCAA Tournament](https://i.turner.ncaa.com/sites/default/files/images/2018/03/11/2018-ncaa-tournament-bracket.jpg)\n",
    "\n",
    "The dataset consists of two files:\n",
    "\n",
    "- `ncaa_team_data.parquet`: a table of team statistics for each team in the dataset.\n",
    "- `ncaa_game_data.parquet`: a table of game results, including the date, location, and outcome of each game.\n",
    "\n",
    "The team data contains a variety of statistics for each team, including the number of games they won and lost, the number of points they scored and allowed, and a variety of other metrics. We can use these as predictors for game outcomes.The game data will be used to fit the model, as it contains the date, location, and outcome of each game.\n",
    "\n",
    "Let's load the data and take a look at the first few rows of each table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "team_data = pl.read_parquet('../data/ncaa_team_data.parquet')\n",
    "team_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_data = pl.read_parquet(\"../data/ncaa_game_data.parquet\")\n",
    "game_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a target variable, we will use the margin of victory for the home team, which is positive if the home team wins and negative if the away team wins.\n",
    "\n",
    "Let's take a look at the distribution of game margins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(game_data, x=\"home_margin\", nbins=40, title=\"Distribution of Game Margins\")\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Game Margin (home)\",\n",
    "    yaxis_title=\"Count\",\n",
    "    bargap=0.1,\n",
    "    width=600\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution looks roughly normal, centered slightly above zero. This reflects the \"home court advantage\" that is common in most sports, particularly basketball.\n",
    "\n",
    "Thus it would seem reasonable to select a Gaussian likelihood for the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = game_data['home_margin'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the set of potential predictor variables available in the team data.\n",
    "\n",
    "The predictor variables used in the model are:\n",
    "\n",
    "- **FG%**: Field Goal Percentage - percentage of field goals made\n",
    "- **3P%**: Three-Point Percentage - percentage of three-point shots made\n",
    "- **FT%**: Free Throw Percentage - percentage of free throws made\n",
    "- **ORB**: Offensive Rebounds - number of rebounds on the offensive end\n",
    "- **TRB**: Total Rebounds - total number of rebounds (offensive + defensive)\n",
    "- **AST**: Assists - number of passes leading directly to a made basket\n",
    "- **STL**: Steals - number of times the ball was taken from the opponent\n",
    "- **BLK**: Blocks - number of shots blocked\n",
    "- **TOV**: Turnovers - number of times the ball was lost to the opponent\n",
    "- **PF**: Personal Fouls - number of fouls committed\n",
    "\n",
    "Since many of these are on different scales, we will standardize them to have mean zero and unit variance. This will help with numerical stability and convergence of the MCMC algorithm, as well as making it easier to interpret the coefficients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_cols = ['FG%',\n",
    " '3P%',\n",
    " 'FT%',\n",
    " 'ORB',\n",
    " 'TRB',\n",
    " 'AST',\n",
    " 'STL',\n",
    " 'BLK',\n",
    " 'TOV',\n",
    " 'PF']\n",
    "\n",
    "# Standardize the predictor columns in polars\n",
    "X = team_data.select(\n",
    "    [(pl.col(c) - pl.col(c).mean()) / pl.col(c).std() for c in predictor_cols]\n",
    ")\n",
    "\n",
    "fig = px.scatter_matrix(X, height=1000, width=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot is encouraging: there is no strong multicollinearity between the predictors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Matchup Model\n",
    "\n",
    "The idea behind this model is that the outcome of a game is determined by the relative strengths of the two teams playing. For each game $g$, we want to predict the score differential (home team score minus away team score). This expectation is characterized by the difference in latent variables representing the strengths of the competing teams:\n",
    "$$ \\delta_g = S_{\\text{home}_g} - S_{\\text{away}_g} $$\n",
    "\n",
    "Also, recall that there is a home court advantage, so we need to account for the fact that the home team is expected to have a slight advantage over the away team. This is represented by the intercept $H$, which is the average home court advantage across all games. Thus, the expected score differential is given by:\n",
    "$$ \\mu_g = H + \\delta_g $$\n",
    "\n",
    "The team strength parameters $S$ will be calculated as a linear combination of the predictor variables.\n",
    "\n",
    "$$ S_t = \\sum_{p \\in \\text{predictors}} \\beta_p \\cdot X_{t,p} $$\n",
    "\n",
    "We will proceed with the assumption that the observed score differential for each game $y_g$, follows a Normal distribution. The mean of this distribution is the `expected_margin` and its standard deviation is the `observation_error` ($\\sigma$):\n",
    "\n",
    "$$ y_g \\sim \\text{Normal}(\\mu_g, \\sigma) $$\n",
    "\n",
    "**Priors**\n",
    "\n",
    "Now that we have defined the general structure of the model, we can choose some priors.\n",
    "\n",
    "For the predictor coefficients $\\beta_p$, we will use a Normal distribution with a mean of 0 and a standard deviation of 100:\n",
    "\n",
    "$$ \\beta_p \\sim \\text{Normal}(0, 10) $$\n",
    "\n",
    "The mean of 0 suggests no initial bias towards positive or negative impact, and a large standard deviation of 10 indicates a weak prior (allowing data to inform the values).\n",
    "\n",
    "The home advantage $H$ is a single, global parameter representing the average point advantage for the home team across all games. Since we are confident that the home advantage is positive, we will use a Half-Normal distribution with a standard deviation of 10:\n",
    "\n",
    "$$ H \\sim \\text{HalfNormal}(10) $$\n",
    "\n",
    "Finally, the standard deviation of the game outcomes is the inherent variability or noise not explained by the predictors. It's drawn from a Half-Cauchy distribution, a common choice for scale parameters as it's restricted to positive values and has heavy tails, allowing for occasional large deviations:\n",
    "\n",
    "$$ \\sigma \\sim \\text{HalfCauchy}(1) $$\n",
    "\n",
    "Let's implement this model in PyMC.\n",
    "\n",
    "---\n",
    "\n",
    "It is not required, but recommended to use named dimensions for the predictor and team variables. This allows us to pass the predictor names as coordinates to the model, which will allow us to use them later when we are working with the model output. We will include the college names, since the predictor variables are school-specific, as well as the predictor variable names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = dict(\n",
    "    predictor=predictor_cols,\n",
    "    team=team_data[\"School\"].to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are stored as a dictionary, which is passed to the `Model` constructor as the `coords` argument.\n",
    "\n",
    "The first part of the model to define are the priors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords=coords) as ncaa_model:\n",
    "    \n",
    "    # Predictor coefficients\n",
    "    beta = pm.Normal('beta', 0, sigma=10, dims=\"predictor\")\n",
    "\n",
    "    # Home advantage\n",
    "    home_advantage = pm.HalfNormal('home_advantage', sigma=10)\n",
    "\n",
    "    # Observation error\n",
    "    sigma = pm.HalfCauchy('sigma', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Transformation\n",
    "\n",
    "To support efficient sampling by PyMC's MCMC algorithms, any continuous variables that are constrained to a sub-interval of the real line are automatically transformed so that their support is unconstrained. This frees sampling algorithms from having to deal with boundary constraints.\n",
    "\n",
    "For example, if we look at the variables we have created in the model so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ncaa_model.value_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's `value_vars` attribute stores the values of each random variable actually used by the model's log-likelihood.\n",
    "\n",
    "As the name suggests, the variables `sigma` and `home_advantage` have been log-transformed, and this is the space over which posterior sampling takes place. When a sample is drawn, the value of the transformed variable is simply back-transformed to recover the original variable.\n",
    "\n",
    "By default, auto-transformed variables are ignored when summarizing and plotting model output, since they are not generally of interest to the user.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deterministic Variables\n",
    "\n",
    "A deterministic variable is one whose values are **completely determined** by the values of their parents.\n",
    "\n",
    "In this model, the team strength parameters are defined as a linear combination of the predictor variables. Thus, we employ a `Deterministic` variable to represent them. Two things happen when a variable is created this way:\n",
    "\n",
    "1. The variable is given a name (passed as the first argument)\n",
    "2. The variable is appended to the model's list of random variables. This will ensure that the variable's values are automatically stored in the model's trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ncaa_model:\n",
    "    \n",
    "    team_strength = pm.Deterministic('team_strength', beta.dot(X.to_numpy().T), dims=\"team\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected margin is then the sum of the home advantage and the difference in team strengths. There is one value for every game in the dataset, so we may not want to store these values in the trace. Hence, we will not pass them to the `Deterministic` constructor but are included as *anonymous* variables; they will be computed on the fly and discarded at each iteration. So, this approach is only appropriate for intermediate values in your model that you do not wish to obtain posterior estimates for, alongside the other variables in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ncaa_model:\n",
    "    \n",
    "    expected_margin = home_advantage + team_strength[game_data['home_team_id'].to_numpy()] - team_strength[game_data['away_team_id'].to_numpy()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observed Random Variables\n",
    "\n",
    "Stochastic random variables whose values are observed are represented by a different class than unobserved random variables. An `ObservedRV` object is instantiated any time a stochastic variable is specified with data passed as the `observed` argument. \n",
    "\n",
    "In our model, the observed game outcomes are represented by an `ObservedRV` using a normal random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ncaa_model:\n",
    "\n",
    "    pm.Normal('outcome', expected_margin, sigma=sigma, observed=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important responsibility of `ObservedRV` is to automatically handle missing values in the data, if there are any."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the graph representation of the final model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(ncaa_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factor Potentials\n",
    "\n",
    "For some applications, we want to be able to modify the joint density by incorporating terms that don't correspond to probabilities of variables conditional on parents. For example, suppose in the IQ drug model we want to constrain the difference between the placebo and drug means to be less than 10, so that the joint density becomes:\n",
    "\n",
    "$$p(y,\\nu,\\mu_1,\\mu_2, \\sigma_1, \\sigma_2) \\propto p(y|\\nu,\\mu_1,\\mu_2, \\sigma_1, \\sigma_2) p(\\nu) p(\\mu_1) p(\\mu_2) p(\\sigma_1) p(\\sigma_2) I(|\\mu_2-\\mu_1| \\lt 10)$$\n",
    "\n",
    "We call such log-probability terms **factor potentials** (Jordan 2004).\n",
    "\n",
    "A potential can be created via the `Potential` function, in a way very similar to `Deterministic`'s named interface:\n",
    "\n",
    "```python\n",
    "with disaster_model:\n",
    "\n",
    "    diff_constraint = pm.Potential('diff_constraint', pm.math.switch(pm.math.abs(treat_mean-placebo_mean)>10, -np.inf, 0))\n",
    "```\n",
    "\n",
    "The function takes just a `name` as its first argument and an expression returning the appropriate log-probability as the second argument.\n",
    "\n",
    "A common use of a factor potential is to represent an observed likelihood, where the **observations are partly a function of model variables**. In the contrived example below, we are representing the error in a linear regression model as a zero-mean normal random variable. Thus, the \"data\" in this scenario is the residual, which is a function both of the data and the regression parameters.\n",
    "\n",
    "If we represent this as a standard likelihood function (a `Distribution` with an `observed` keyword argument), we run into problems. This parameterization would not be compatible with an observed stochastic, because the `err` term would become fixed in the likelihood and not be allowed to change during sampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vals = np.array([15, 10, 16, 11, 9, 11, 10, 18, 11])\n",
    "x_vals = np.array([1, 2, 4, 5, 6, 8, 19, 18, 12])\n",
    "\n",
    "with pm.Model() as regression:\n",
    "    sigma = pm.HalfCauchy(\"sigma\", 5)\n",
    "    beta = pm.Normal(\"beta\", 0, sigma=2)\n",
    "    mu = pm.Normal(\"mu\", 0, sigma=10)\n",
    "\n",
    "    err = y_vals - (mu + beta * x_vals)\n",
    "\n",
    "    like = pm.Normal(\"like\", 0, sigma=sigma, observed=err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we can re-express the likelihood as a factor potential, which is a function of the data and the model parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as regression:\n",
    "    sigma = pm.HalfCauchy(\"sigma\", 5)\n",
    "    beta = pm.Normal(\"beta\", 0, sigma=2)\n",
    "    mu = pm.Normal(\"mu\", 0, sigma=10)\n",
    "\n",
    "    err = y_vals - (mu + beta * x_vals)\n",
    "\n",
    "    like = pm.Potential(\"like\", pm.logp(pm.Normal.dist(0, sigma=sigma), err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Bioassay Model\n",
    "\n",
    "Let's return to the logistic regression model that we previously hand-coded in PyTensor.\n",
    "\n",
    "If you recall, we are trying to predict the number of deaths in an acute toxicity test, which we model as a binomial response $y$, with the probability of death being a linear function of log-dose ($x$):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{logit}(p_i) &= a + b x_i \\\\\n",
    "y_i &\\sim \\text{Bin}(n_i, p_i) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now construct this model using PyMC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log dose in each group\n",
    "log_dose = [-0.86, -0.3, -0.05, 0.73]\n",
    "\n",
    "# Sample size in each group\n",
    "n = 5\n",
    "\n",
    "# Outcomes\n",
    "deaths = [0, 1, 3, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. Ching & Chen. 2007. Transitional Markov chain Monte Carlo method for Bayesian model updating, model class selection and model averaging. Journal of Engineering Mechanics 2007\n",
    "2. Hoffman MD, Gelman A. 2014. The No-U-turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. The Journal of Machine Learning Research. 15(1):1593-1623.\n",
    "3. M.I. Jordan. 2004. Graphical models. Statist. Sci., 19(1):140–155.\n",
    "4. Neal, R. M. 2003. Slice sampling. The Annals of Statistics, 31(3), 705–767. doi:10.1111/1467-9868.00198\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
